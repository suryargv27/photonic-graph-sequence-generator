{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c249caeb-8388-4b47-93fe-1fc2e891186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "# Set device for acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# =========================\n",
    "# Graph Environment\n",
    "# =========================\n",
    "class GraphEnv:\n",
    "    def __init__(self, A, T, t1=0.1, t2=0.1, t3=10, b=0.5):\n",
    "        self.A = A.copy()\n",
    "        self.T = T.copy()\n",
    "        self.n = len(T)\n",
    "        self.t1, self.t2, self.t3, self.b = t1, t2, t3, b\n",
    "        self.done = False\n",
    "\n",
    "    def clone(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def get_neighbors(self, i):\n",
    "        return set(np.where(self.A[i] == 1)[0])\n",
    "\n",
    "    def delete_node(self, i):\n",
    "        self.A[i, :] = 0\n",
    "        self.A[:, i] = 0\n",
    "        self.T[i] = 0\n",
    "\n",
    "    def action_cost(self, a):\n",
    "        if a == 1: return self.t1 + 4*self.t2 + self.b*self.t3\n",
    "        if a == 2: return self.t1 + self.t2\n",
    "        if a == 3: return self.t1 + self.t2\n",
    "        if a == 4: return self.t1 + 3*self.t2\n",
    "        if a == 5: return self.t3\n",
    "        if a == 6: return 3*self.t2 + self.t3\n",
    "        return 0\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        a, nodes = action\n",
    "        cost = self.action_cost(a)\n",
    "        \n",
    "        if a == 1:\n",
    "            self.T[nodes] = 1\n",
    "        elif a == 2:\n",
    "            i, j = nodes\n",
    "            self.T[j] = 1\n",
    "            self.delete_node(i)\n",
    "        elif a in {3, 4, 6}:\n",
    "            i, _ = nodes\n",
    "            self.delete_node(i)\n",
    "        elif a == 5:\n",
    "            i, j = nodes\n",
    "            self.A[i, j] = self.A[j, i] = 0\n",
    "\n",
    "        reward = -cost\n",
    "        \n",
    "        self.done = (\n",
    "                np.all(self.T != -1) # All nodes are either 0 (deleted) or 1 (processed)\n",
    "                or np.all(self.A.sum(axis=0) == 0)\n",
    "                or len(self.get_valid_actions()) == 0\n",
    "            )\n",
    "        return reward, self.done\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        acts = []\n",
    "        n = self.n\n",
    "        for i in range(n):\n",
    "            if self.T[i] == -1:\n",
    "                acts.append((1, i))\n",
    "            if self.T[i] == 1:\n",
    "                nbrs = self.get_neighbors(i)\n",
    "                if len(nbrs) == 1:\n",
    "                    j = next(iter(nbrs))\n",
    "                    if self.T[j] == -1: acts.append((2, (i, j)))\n",
    "            if self.T[i] == -1:\n",
    "                nbrs = self.get_neighbors(i)\n",
    "                if len(nbrs) == 1:\n",
    "                    j = next(iter(nbrs))\n",
    "                    if self.T[j] == 1: acts.append((3, (i, j)))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if self.T[i] == -1 and self.T[j] == 1:\n",
    "                    if self.get_neighbors(i) == self.get_neighbors(j):\n",
    "                        acts.append((4, (i, j)))\n",
    "                if self.T[i] == 1 and self.T[j] == 1:\n",
    "                    if self.A[i, j] == 1: acts.append((5, (i, j)))\n",
    "                    if self.get_neighbors(i) == self.get_neighbors(j):\n",
    "                        acts.append((6, (i, j)))\n",
    "        return acts\n",
    "\n",
    "    def get_state(self):\n",
    "        return np.concatenate([self.A.flatten(), self.T])\n",
    "\n",
    "# =========================\n",
    "# Shared Action Encoding\n",
    "# =========================\n",
    "def encode_action(action, n):\n",
    "    a, nodes = action\n",
    "    v = np.zeros(2*n + 6)\n",
    "    v[a-1] = 1\n",
    "    if isinstance(nodes, (int, np.integer)):\n",
    "        v[6 + nodes] = 1\n",
    "    else:\n",
    "        i, j = nodes\n",
    "        v[6 + i] = 1\n",
    "        v[6 + n + j] = 1\n",
    "    return v\n",
    "\n",
    "# =========================\n",
    "# Q Network\n",
    "# =========================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# =========================\n",
    "# Optimized Training\n",
    "# =========================\n",
    "def train(env, episodes=300, batch_size=64, gamma=0.99, lr=1e-3):\n",
    "    n = env.n\n",
    "    state_dim = n**2 + n\n",
    "    input_dim = state_dim + 2*n + 6\n",
    "\n",
    "    q_net = DQN(input_dim).to(device)\n",
    "    target_net = DQN(input_dim).to(device)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    \n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=20000)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    eps_start, eps_end, eps_decay = 1.0, 0.05, 0.992\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        e = env.clone()\n",
    "        state = e.get_state()\n",
    "        eps = max(eps_end, eps_start * (eps_decay ** ep))\n",
    "        ep_reward = 0\n",
    "\n",
    "        while True:\n",
    "            acts = e.get_valid_actions()\n",
    "            if not acts: break\n",
    "\n",
    "            # Epsilon-Greedy Selection\n",
    "            if random.random() < eps:\n",
    "                action = random.choice(acts)\n",
    "            else:\n",
    "                q_net.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Batch predict all valid actions for current state\n",
    "                    st_tensor = torch.tensor(state, dtype=torch.float32).repeat(len(acts), 1)\n",
    "                    ac_tensor = torch.tensor([encode_action(a, n) for a in acts], dtype=torch.float32)\n",
    "                    inputs = torch.cat([st_tensor, ac_tensor], dim=1).to(device)\n",
    "                    qs = q_net(inputs)\n",
    "                    action = acts[torch.argmax(qs).item()]\n",
    "                q_net.train()\n",
    "\n",
    "            reward, done = e.apply_action(action)\n",
    "            next_state = e.get_state()\n",
    "            next_acts = e.get_valid_actions()\n",
    "            \n",
    "            memory.append((state, action, reward, next_state, next_acts, done))\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "\n",
    "            # Vectorized Learning Step\n",
    "            if len(memory) >= batch_size:\n",
    "                batch = random.sample(memory, batch_size)\n",
    "                s_b, a_b, r_b, ns_b, na_b, d_b = zip(*batch)\n",
    "\n",
    "                # 1. Current Q values\n",
    "                s_t = torch.tensor(np.stack(s_b), dtype=torch.float32)\n",
    "                a_t = torch.tensor(np.stack([encode_action(a, n) for a in a_b]), dtype=torch.float32)\n",
    "                curr_inputs = torch.cat([s_t, a_t], dim=1).to(device)\n",
    "                curr_q = q_net(curr_inputs).squeeze()\n",
    "\n",
    "                # 2. Target Q values (Vectorized where possible)\n",
    "                target_q = torch.zeros(batch_size, device=device)\n",
    "                with torch.no_grad():\n",
    "                    for i in range(batch_size):\n",
    "                        if d_b[i] or not na_b[i]:\n",
    "                            target_q[i] = r_b[i]\n",
    "                        else:\n",
    "                            # Sub-batch for next actions\n",
    "                            ns_rep = torch.tensor(ns_b[i], dtype=torch.float32).repeat(len(na_b[i]), 1)\n",
    "                            na_enc = torch.tensor([encode_action(a2, n) for a2 in na_b[i]], dtype=torch.float32)\n",
    "                            next_inputs = torch.cat([ns_rep, na_enc], dim=1).to(device)\n",
    "                            max_next_q = torch.max(target_net(next_inputs))\n",
    "                            target_q[i] = r_b[i] + gamma * max_next_q\n",
    "\n",
    "                loss = criterion(curr_q, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if done: break\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    return q_net\n",
    "\n",
    "# =========================\n",
    "# Fixed Inference\n",
    "# =========================\n",
    "def infer_dqn(env, trained_qnet):\n",
    "    env_copy = env.clone()\n",
    "    trained_qnet.eval()\n",
    "    trained_qnet.to(device)\n",
    "    \n",
    "    total_cost = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while True:\n",
    "        valid_actions = env_copy.get_valid_actions()\n",
    "        if not valid_actions: break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = env_copy.get_state()\n",
    "            st_tensor = torch.tensor(state, dtype=torch.float32).repeat(len(valid_actions), 1)\n",
    "            ac_tensor = torch.tensor([encode_action(a, env_copy.n) for a in valid_actions], dtype=torch.float32)\n",
    "            inputs = torch.cat([st_tensor, ac_tensor], dim=1).to(device)\n",
    "            \n",
    "            qs = trained_qnet(inputs)\n",
    "            best_idx = torch.argmax(qs).item()\n",
    "            best_action = valid_actions[best_idx]\n",
    "        \n",
    "        reward, done = env_copy.apply_action(best_action)\n",
    "        # Re-extract the actual cost from the shaped reward for reporting\n",
    "        actual_step_cost = env_copy.action_cost(best_action[0])\n",
    "        total_cost += actual_step_cost\n",
    "        steps += 1\n",
    "        \n",
    "        if done: break\n",
    "\n",
    "    t_processed = int(np.sum(env_copy.T == 1))\n",
    "    return steps, t_processed, total_cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a443da6a-632a-4e93-8409-24e8938ca81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_solve(env):\n",
    "    env_copy = env.clone()\n",
    "    steps = 0\n",
    "    total_cost = 0\n",
    "    \n",
    "    while True:\n",
    "        valid_actions = env_copy.get_valid_actions()\n",
    "        if not valid_actions:\n",
    "            break\n",
    "\n",
    "        # Choose action with minimum immediate cost\n",
    "        # We use a lambda to sort by the action_cost of the action type (action[0])\n",
    "        best_action = min(valid_actions, key=lambda act: env_copy.action_cost(act[0]))\n",
    "\n",
    "        # Track the actual cost before applying any shaping\n",
    "        actual_cost = env_copy.action_cost(best_action[0])\n",
    "        total_cost += actual_cost\n",
    "        \n",
    "        _, done = env_copy.apply_action(best_action)\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    t_processed = int(np.sum(env_copy.T == 1))\n",
    "    return steps, t_processed, total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2013fe82-b914-4909-8bb4-87cf001cb462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_163784/2347620496.py:168: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /build/python-pytorch/src/pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
      "  ac_tensor = torch.tensor([encode_action(a, n) for a in acts], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done\n",
      "Results\n",
      "DQN     Time: 10, Emitter: 4, cost: 54.0\n",
      "Greedy  Time: 10, Emitter: 5, cost: 60.0\n",
      "Training start\n",
      "Training done\n",
      "Results\n",
      "DQN     Time: 10, Emitter: 10, cost: 100.0\n",
      "Greedy  Time: 10, Emitter: 10, cost: 100.0\n",
      "Training start\n",
      "Training done\n",
      "Results\n",
      "DQN     Time: 10, Emitter: 9, cost: 92.0\n",
      "Greedy  Time: 10, Emitter: 9, cost: 92.0\n",
      "Training start\n",
      "Training done\n",
      "Results\n",
      "DQN     Time: 11, Emitter: 5, cost: 72.0\n",
      "Greedy  Time: 10, Emitter: 7, cost: 76.0\n",
      "Training start\n",
      "Training done\n",
      "Results\n",
      "DQN     Time: 10, Emitter: 9, cost: 92.0\n",
      "Greedy  Time: 10, Emitter: 9, cost: 92.0\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def random_graph(n, p):\n",
    "    G = nx.erdos_renyi_graph(n, p)\n",
    "    A = nx.to_numpy_array(G, dtype=int)\n",
    "    return A\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(5):\n",
    "        n = 10\n",
    "        A = random_graph(n,p=0.3)\n",
    "        T = np.array([-1] * n)\n",
    "        env = GraphEnv(A, T, t1=1, t2=1, t3=10,b=0.5)\n",
    "    \n",
    "        print(\"Training start\")\n",
    "        trained_model = train(env, episodes=300)\n",
    "        print(\"Training done\")\n",
    "    \n",
    "        dqn_steps, dqn_ones, dqn_cost = infer_dqn(env, trained_model)\n",
    "        greedy_steps, greedy_ones, greedy_cost = greedy_solve(env)\n",
    "    \n",
    "        print(\"Results\")\n",
    "        print(f\"DQN     Time: {dqn_steps}, Emitter: {dqn_ones}, cost: {dqn_cost}\")\n",
    "        print(f\"Greedy  Time: {greedy_steps}, Emitter: {greedy_ones}, cost: {greedy_cost}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
