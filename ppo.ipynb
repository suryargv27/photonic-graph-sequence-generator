{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8efa0c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =========================\n",
    "# Constants\n",
    "# =========================\n",
    "t1, t2, t3, b = 0.1, 0.1, 10.0, 0.5\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "PPO_EPOCHS = 4\n",
    "BATCH_SIZE = 256\n",
    "LR = 3e-4\n",
    "\n",
    "# =========================\n",
    "# Environment\n",
    "# =========================\n",
    "class GraphEnv:\n",
    "    def __init__(self, A, T):\n",
    "        self.A = A.copy()\n",
    "        self.T = T.copy()\n",
    "        self.n = len(T)\n",
    "\n",
    "    def clone(self):\n",
    "        return GraphEnv(self.A.copy(), self.T.copy())\n",
    "\n",
    "    def done(self):\n",
    "        return np.all(self.A == 0) and np.all(np.isin(self.T, [0, 1]))\n",
    "\n",
    "    def get_neighbors(self, i):\n",
    "        return set(np.where(self.A[i] == 1)[0])\n",
    "\n",
    "    def delete(self, i):\n",
    "        self.T[i] = 0\n",
    "        self.A[i, :] = 0\n",
    "        self.A[:, i] = 0\n",
    "\n",
    "    def step(self, action):\n",
    "        a, i, j = action\n",
    "        cost = 0.0\n",
    "\n",
    "        if a == 1:\n",
    "            self.T[i] = 1\n",
    "            cost = t1 + 4*t2 + b*t3\n",
    "        elif a == 2:\n",
    "            self.T[j] = 1\n",
    "            self.delete(i)\n",
    "            cost = t1 + t2\n",
    "        elif a == 3:\n",
    "            self.delete(i)\n",
    "            cost = t1 + t2\n",
    "        elif a == 4:\n",
    "            self.delete(i)\n",
    "            cost = t1 + 3*t2\n",
    "        elif a == 5:\n",
    "            self.A[i, j] = 0\n",
    "            self.A[j, i] = 0\n",
    "            cost = t3\n",
    "        elif a == 6:\n",
    "            self.delete(i)\n",
    "            cost = 3*t2 + t3\n",
    "\n",
    "        reward = -cost\n",
    "        return reward, self.done()\n",
    "\n",
    "# =========================\n",
    "# Action Enumeration\n",
    "# =========================\n",
    "def enumerate_actions(env):\n",
    "    actions = []\n",
    "    n = env.n\n",
    "\n",
    "    for i in range(n):\n",
    "        Ni = env.get_neighbors(i)\n",
    "\n",
    "        if env.T[i] == -1:\n",
    "            actions.append((1, i, -1))\n",
    "\n",
    "        if env.T[i] == 1 and len(Ni) == 1:\n",
    "            j = next(iter(Ni))\n",
    "            if env.T[j] == -1:\n",
    "                actions.append((2, i, j))\n",
    "\n",
    "        if env.T[i] == -1 and len(Ni) == 1:\n",
    "            j = next(iter(Ni))\n",
    "            if env.T[j] == 1:\n",
    "                actions.append((3, i, j))\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if env.get_neighbors(i) == env.get_neighbors(j):\n",
    "                if env.T[i] == -1 and env.T[j] == 1:\n",
    "                    actions.append((4, i, j))\n",
    "                if env.T[i] == 1 and env.T[j] == 1:\n",
    "                    actions.append((6, i, j))\n",
    "            if env.T[i] == 1 and env.T[j] == 1 and env.A[i, j] == 1:\n",
    "                actions.append((5, i, j))\n",
    "\n",
    "    return list(set(actions))\n",
    "\n",
    "# =========================\n",
    "# GNN Encoder (GIN)\n",
    "# =========================\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        nn1 = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 128))\n",
    "        nn2 = nn.Sequential(nn.Linear(128, 128), nn.ReLU(), nn.Linear(128, 128))\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.conv2 = GINConv(nn2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        g = global_mean_pool(x, batch)\n",
    "        return g, x\n",
    "\n",
    "# =========================\n",
    "# Policy + Value Networks\n",
    "# =========================\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(128 + 6 + 128 + 128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, g, ai):\n",
    "        g = g.expand(ai.size(0), -1)\n",
    "        return self.mlp(torch.cat([g, ai], dim=1)).squeeze(-1)\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, g):\n",
    "        return self.mlp(g).squeeze(-1)\n",
    "\n",
    "# =========================\n",
    "# Utilities\n",
    "# =========================\n",
    "def encode_graph(A, T):\n",
    "    edge_index = torch.tensor(np.array(np.nonzero(A)), dtype=torch.long)\n",
    "    x = torch.zeros((len(T), 3))\n",
    "    for i, t in enumerate(T):\n",
    "        x[i, t+1] = 1\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "def encode_action(action, node_emb):\n",
    "    a, i, j = action\n",
    "    a_onehot = F.one_hot(torch.tensor(a-1), 6).float()\n",
    "    ei = node_emb[i]\n",
    "    ej = node_emb[j] if j >= 0 else torch.zeros_like(ei)\n",
    "    return torch.cat([a_onehot, ei, ej])\n",
    "\n",
    "# =========================\n",
    "# PPO Training Loop\n",
    "# =========================\n",
    "def train(envs, episodes=300):\n",
    "    encoder = GINEncoder().to(device)\n",
    "    policy = PolicyNet().to(device)\n",
    "    value_net = ValueNet().to(device)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) +\n",
    "        list(policy.parameters()) +\n",
    "        list(value_net.parameters()),\n",
    "        lr=3e-4\n",
    "    )\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        memory = []\n",
    "\n",
    "        # =========================\n",
    "        # Rollout (NO GRAD)\n",
    "        # =========================\n",
    "        with torch.no_grad():\n",
    "            for env in envs:\n",
    "                env = env.clone()\n",
    "\n",
    "                while not env.done():\n",
    "                    actions = enumerate_actions(env)\n",
    "                    if not actions:\n",
    "                        break\n",
    "\n",
    "                    data = encode_graph(env.A, env.T)\n",
    "                    batch = Batch.from_data_list([data]).to(device)\n",
    "\n",
    "                    g, node_emb = encoder(batch)\n",
    "\n",
    "                    action_embs = torch.stack(\n",
    "                        [encode_action(a, node_emb) for a in actions]\n",
    "                    ).to(device)\n",
    "\n",
    "                    logits = policy(g, action_embs)\n",
    "                    probs = F.softmax(logits, dim=0)\n",
    "                    dist = torch.distributions.Categorical(probs)\n",
    "\n",
    "                    idx = dist.sample()\n",
    "                    action = actions[idx.item()]\n",
    "\n",
    "                    reward, done = env.step(action)\n",
    "\n",
    "                    memory.append({\n",
    "                        \"A\": env.A.copy(),\n",
    "                        \"T\": env.T.copy(),\n",
    "                        \"action\": action,\n",
    "                        \"old_log_prob\": dist.log_prob(idx).detach(),\n",
    "                        \"reward\": reward,\n",
    "                        \"done\": done\n",
    "                    })\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "        # =========================\n",
    "        # Compute returns\n",
    "        # =========================\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for step in reversed(memory):\n",
    "            G = step[\"reward\"] + GAMMA * G\n",
    "            returns.insert(0, G)\n",
    "\n",
    "        returns = torch.tensor(returns, dtype=torch.float32).to(device)\n",
    "\n",
    "        # =========================\n",
    "        # PPO UPDATE (RECOMPUTE EVERYTHING)\n",
    "        # =========================\n",
    "        for _ in range(PPO_EPOCHS):\n",
    "            total_loss = 0.0\n",
    "\n",
    "            for step, R in zip(memory, returns):\n",
    "                data = encode_graph(step[\"A\"], step[\"T\"])\n",
    "                batch = Batch.from_data_list([data]).to(device)\n",
    "\n",
    "                g, node_emb = encoder(batch)\n",
    "\n",
    "                action = step[\"action\"]\n",
    "                action_emb = encode_action(action, node_emb).unsqueeze(0).to(device)\n",
    "\n",
    "                logits = policy(g, action_emb)\n",
    "                log_prob = F.log_softmax(logits, dim=0)[0]\n",
    "\n",
    "                ratio = torch.exp(log_prob - step[\"old_log_prob\"])\n",
    "\n",
    "                advantage = R - value_net(g).detach()\n",
    "\n",
    "                surr1 = ratio * advantage\n",
    "                surr2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advantage\n",
    "                policy_loss = -torch.min(surr1, surr2)\n",
    "\n",
    "                value_loss = F.mse_loss(value_net(g), R.unsqueeze(0))\n",
    "\n",
    "                total_loss += policy_loss + value_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Episode {ep} finished\")\n",
    "\n",
    "    return encoder, policy, value_net\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6857ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def run_inference(env, encoder, policy, deterministic=True, max_steps=1000):\n",
    "    \"\"\"\n",
    "    env          : GraphEnv\n",
    "    encoder      : trained GINEncoder\n",
    "    policy       : trained PolicyNet\n",
    "    deterministic: if True -> greedy, else sample\n",
    "    max_steps    : safety cap\n",
    "    \"\"\"\n",
    "\n",
    "    encoder.eval()\n",
    "    policy.eval()\n",
    "\n",
    "    total_reward = 0.0\n",
    "    steps = 0\n",
    "    trajectory = []\n",
    "\n",
    "    while not env.done() and steps < max_steps:\n",
    "        actions = enumerate_actions(env)\n",
    "        if not actions:\n",
    "            break\n",
    "\n",
    "        # Encode graph\n",
    "        data = encode_graph(env.A, env.T)\n",
    "        batch = Batch.from_data_list([data]).to(device)\n",
    "\n",
    "        g, node_emb = encoder(batch)\n",
    "\n",
    "        # Encode actions\n",
    "        action_embs = torch.stack(\n",
    "            [encode_action(a, node_emb) for a in actions]\n",
    "        ).to(device)\n",
    "\n",
    "        # Policy logits\n",
    "        logits = policy(g, action_embs)\n",
    "        probs = F.softmax(logits, dim=0)\n",
    "\n",
    "        if deterministic:\n",
    "            idx = torch.argmax(probs)\n",
    "        else:\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            idx = dist.sample()\n",
    "\n",
    "        action = actions[idx.item()]\n",
    "        reward, done = env.step(action)\n",
    "\n",
    "        total_reward += reward\n",
    "        trajectory.append((action, reward))\n",
    "\n",
    "        steps += 1\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return {\n",
    "        \"total_reward\": total_reward,\n",
    "        \"steps\": steps,\n",
    "        \"trajectory\": trajectory,\n",
    "        \"done\": env.done()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff9ddb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training graphs...\n",
      "Train graph 0: n=11\n",
      "Train graph 1: n=12\n",
      "Train graph 2: n=8\n",
      "Train graph 3: n=9\n",
      "Train graph 4: n=12\n",
      "\n",
      "Training PPO...\n",
      "Episode 0 finished\n",
      "Episode 1 finished\n",
      "Episode 2 finished\n",
      "Episode 3 finished\n",
      "Episode 4 finished\n",
      "Episode 5 finished\n",
      "Episode 6 finished\n",
      "Episode 7 finished\n",
      "Episode 8 finished\n",
      "Episode 9 finished\n",
      "Episode 10 finished\n",
      "Episode 11 finished\n",
      "Episode 12 finished\n",
      "Episode 13 finished\n",
      "Episode 14 finished\n",
      "Episode 15 finished\n",
      "Episode 16 finished\n",
      "Episode 17 finished\n",
      "Episode 18 finished\n",
      "Episode 19 finished\n",
      "Episode 20 finished\n",
      "Episode 21 finished\n",
      "Episode 22 finished\n",
      "Episode 23 finished\n",
      "Episode 24 finished\n",
      "Episode 25 finished\n",
      "Episode 26 finished\n",
      "Episode 27 finished\n",
      "Episode 28 finished\n",
      "Episode 29 finished\n",
      "Episode 30 finished\n",
      "Episode 31 finished\n",
      "Episode 32 finished\n",
      "Episode 33 finished\n",
      "Episode 34 finished\n",
      "Episode 35 finished\n",
      "Episode 36 finished\n",
      "Episode 37 finished\n",
      "Episode 38 finished\n",
      "Episode 39 finished\n",
      "Episode 40 finished\n",
      "Episode 41 finished\n",
      "Episode 42 finished\n",
      "Episode 43 finished\n",
      "Episode 44 finished\n",
      "Episode 45 finished\n",
      "Episode 46 finished\n",
      "Episode 47 finished\n",
      "Episode 48 finished\n",
      "Episode 49 finished\n",
      "Episode 50 finished\n",
      "Episode 51 finished\n",
      "Episode 52 finished\n",
      "Episode 53 finished\n",
      "Episode 54 finished\n",
      "Episode 55 finished\n",
      "Episode 56 finished\n",
      "Episode 57 finished\n",
      "Episode 58 finished\n",
      "Episode 59 finished\n",
      "Episode 60 finished\n",
      "Episode 61 finished\n",
      "Episode 62 finished\n",
      "Episode 63 finished\n",
      "Episode 64 finished\n",
      "Episode 65 finished\n",
      "Episode 66 finished\n",
      "Episode 67 finished\n",
      "Episode 68 finished\n",
      "Episode 69 finished\n",
      "Episode 70 finished\n",
      "Episode 71 finished\n",
      "Episode 72 finished\n",
      "Episode 73 finished\n",
      "Episode 74 finished\n",
      "Episode 75 finished\n",
      "Episode 76 finished\n",
      "Episode 77 finished\n",
      "Episode 78 finished\n",
      "Episode 79 finished\n",
      "Episode 80 finished\n",
      "Episode 81 finished\n",
      "Episode 82 finished\n",
      "Episode 83 finished\n",
      "Episode 84 finished\n",
      "Episode 85 finished\n",
      "Episode 86 finished\n",
      "Episode 87 finished\n",
      "Episode 88 finished\n",
      "Episode 89 finished\n",
      "Episode 90 finished\n",
      "Episode 91 finished\n",
      "Episode 92 finished\n",
      "Episode 93 finished\n",
      "Episode 94 finished\n",
      "Episode 95 finished\n",
      "Episode 96 finished\n",
      "Episode 97 finished\n",
      "Episode 98 finished\n",
      "Episode 99 finished\n",
      "Episode 100 finished\n",
      "Episode 101 finished\n",
      "Episode 102 finished\n",
      "Episode 103 finished\n",
      "Episode 104 finished\n",
      "Episode 105 finished\n",
      "Episode 106 finished\n",
      "Episode 107 finished\n",
      "Episode 108 finished\n",
      "Episode 109 finished\n",
      "Episode 110 finished\n",
      "Episode 111 finished\n",
      "Episode 112 finished\n",
      "Episode 113 finished\n",
      "Episode 114 finished\n",
      "Episode 115 finished\n",
      "Episode 116 finished\n",
      "Episode 117 finished\n",
      "Episode 118 finished\n",
      "Episode 119 finished\n",
      "Episode 120 finished\n",
      "Episode 121 finished\n",
      "Episode 122 finished\n",
      "Episode 123 finished\n",
      "Episode 124 finished\n",
      "Episode 125 finished\n",
      "Episode 126 finished\n",
      "Episode 127 finished\n",
      "Episode 128 finished\n",
      "Episode 129 finished\n",
      "Episode 130 finished\n",
      "Episode 131 finished\n",
      "Episode 132 finished\n",
      "Episode 133 finished\n",
      "Episode 134 finished\n",
      "Episode 135 finished\n",
      "Episode 136 finished\n",
      "Episode 137 finished\n",
      "Episode 138 finished\n",
      "Episode 139 finished\n",
      "Episode 140 finished\n",
      "Episode 141 finished\n",
      "Episode 142 finished\n",
      "Episode 143 finished\n",
      "Episode 144 finished\n",
      "Episode 145 finished\n",
      "Episode 146 finished\n",
      "Episode 147 finished\n",
      "Episode 148 finished\n",
      "Episode 149 finished\n",
      "Episode 150 finished\n",
      "Episode 151 finished\n",
      "Episode 152 finished\n",
      "Episode 153 finished\n",
      "Episode 154 finished\n",
      "Episode 155 finished\n",
      "Episode 156 finished\n",
      "Episode 157 finished\n",
      "Episode 158 finished\n",
      "Episode 159 finished\n",
      "Episode 160 finished\n",
      "Episode 161 finished\n",
      "Episode 162 finished\n",
      "Episode 163 finished\n",
      "Episode 164 finished\n",
      "Episode 165 finished\n",
      "Episode 166 finished\n",
      "Episode 167 finished\n",
      "Episode 168 finished\n",
      "Episode 169 finished\n",
      "Episode 170 finished\n",
      "Episode 171 finished\n",
      "Episode 172 finished\n",
      "Episode 173 finished\n",
      "Episode 174 finished\n",
      "Episode 175 finished\n",
      "Episode 176 finished\n",
      "Episode 177 finished\n",
      "Episode 178 finished\n",
      "Episode 179 finished\n",
      "Episode 180 finished\n",
      "Episode 181 finished\n",
      "Episode 182 finished\n",
      "Episode 183 finished\n",
      "Episode 184 finished\n",
      "Episode 185 finished\n",
      "Episode 186 finished\n",
      "Episode 187 finished\n",
      "Episode 188 finished\n",
      "Episode 189 finished\n",
      "Episode 190 finished\n",
      "Episode 191 finished\n",
      "Episode 192 finished\n",
      "Episode 193 finished\n",
      "Episode 194 finished\n",
      "Episode 195 finished\n",
      "Episode 196 finished\n",
      "Episode 197 finished\n",
      "Episode 198 finished\n",
      "Episode 199 finished\n",
      "Episode 200 finished\n",
      "Episode 201 finished\n",
      "Episode 202 finished\n",
      "Episode 203 finished\n",
      "Episode 204 finished\n",
      "Episode 205 finished\n",
      "Episode 206 finished\n",
      "Episode 207 finished\n",
      "Episode 208 finished\n",
      "Episode 209 finished\n",
      "Episode 210 finished\n",
      "Episode 211 finished\n",
      "Episode 212 finished\n",
      "Episode 213 finished\n",
      "Episode 214 finished\n",
      "Episode 215 finished\n",
      "Episode 216 finished\n",
      "Episode 217 finished\n",
      "Episode 218 finished\n",
      "Episode 219 finished\n",
      "Episode 220 finished\n",
      "Episode 221 finished\n",
      "Episode 222 finished\n",
      "Episode 223 finished\n",
      "Episode 224 finished\n",
      "Episode 225 finished\n",
      "Episode 226 finished\n",
      "Episode 227 finished\n",
      "Episode 228 finished\n",
      "Episode 229 finished\n",
      "Episode 230 finished\n",
      "Episode 231 finished\n",
      "Episode 232 finished\n",
      "Episode 233 finished\n",
      "Episode 234 finished\n",
      "Episode 235 finished\n",
      "Episode 236 finished\n",
      "Episode 237 finished\n",
      "Episode 238 finished\n",
      "Episode 239 finished\n",
      "Episode 240 finished\n",
      "Episode 241 finished\n",
      "Episode 242 finished\n",
      "Episode 243 finished\n",
      "Episode 244 finished\n",
      "Episode 245 finished\n",
      "Episode 246 finished\n",
      "Episode 247 finished\n",
      "Episode 248 finished\n",
      "Episode 249 finished\n",
      "Episode 250 finished\n",
      "Episode 251 finished\n",
      "Episode 252 finished\n",
      "Episode 253 finished\n",
      "Episode 254 finished\n",
      "Episode 255 finished\n",
      "Episode 256 finished\n",
      "Episode 257 finished\n",
      "Episode 258 finished\n",
      "Episode 259 finished\n",
      "Episode 260 finished\n",
      "Episode 261 finished\n",
      "Episode 262 finished\n",
      "Episode 263 finished\n",
      "Episode 264 finished\n",
      "Episode 265 finished\n",
      "Episode 266 finished\n",
      "Episode 267 finished\n",
      "Episode 268 finished\n",
      "Episode 269 finished\n",
      "Episode 270 finished\n",
      "Episode 271 finished\n",
      "Episode 272 finished\n",
      "Episode 273 finished\n",
      "Episode 274 finished\n",
      "Episode 275 finished\n",
      "Episode 276 finished\n",
      "Episode 277 finished\n",
      "Episode 278 finished\n",
      "Episode 279 finished\n",
      "Episode 280 finished\n",
      "Episode 281 finished\n",
      "Episode 282 finished\n",
      "Episode 283 finished\n",
      "Episode 284 finished\n",
      "Episode 285 finished\n",
      "Episode 286 finished\n",
      "Episode 287 finished\n",
      "Episode 288 finished\n",
      "Episode 289 finished\n",
      "Episode 290 finished\n",
      "Episode 291 finished\n",
      "Episode 292 finished\n",
      "Episode 293 finished\n",
      "Episode 294 finished\n",
      "Episode 295 finished\n",
      "Episode 296 finished\n",
      "Episode 297 finished\n",
      "Episode 298 finished\n",
      "Episode 299 finished\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# =========================\n",
    "# 0. Reproducibility\n",
    "# =========================\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# =========================\n",
    "# 1. Load fixed graphs\n",
    "# =========================\n",
    "with open(\"fixed_graph_dataset.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "TRAIN_GRAPHS = data[\"train\"]\n",
    "TEST_GRAPHS  = data[\"test\"]\n",
    "\n",
    "def make_envs(graphs):\n",
    "    # IMPORTANT: copy so envs don't share state\n",
    "    return [GraphEnv(A.copy(), T.copy()) for A, T in graphs]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Training graphs (FIXED)\n",
    "# =========================\n",
    "train_envs = make_envs(TRAIN_GRAPHS)\n",
    "\n",
    "print(\"Creating training graphs...\")\n",
    "for k, (A, _) in enumerate(TRAIN_GRAPHS):\n",
    "    print(f\"Train graph {k}: n={A.shape[0]}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. Train PPO\n",
    "# =========================\n",
    "print(\"\\nTraining PPO...\")\n",
    "encoder, policy, value = train(\n",
    "    train_envs,\n",
    "    episodes=300\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1f26aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference on fixed test graphs...\n",
      "\n",
      "Test Graph 0\n",
      "  n: 10\n",
      "  Steps: 19\n",
      "  Total reward: -113.80000000000001\n",
      "  Final T: [0 0 0 1 0 0 0 1 0 0]\n",
      "\n",
      "Test Graph 1\n",
      "  n: 10\n",
      "  Steps: 22\n",
      "  Total reward: -148.79999999999995\n",
      "  Final T: [0 1 0 0 0 1 0 1 1 0]\n",
      "\n",
      "Test Graph 2\n",
      "  n: 14\n",
      "  Steps: 40\n",
      "  Total reward: -307.0\n",
      "  Final T: [0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      "\n",
      "Test Graph 3\n",
      "  n: 14\n",
      "  Steps: 24\n",
      "  Total reward: -124.80000000000003\n",
      "  Final T: [0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
      "\n",
      "Test Graph 4\n",
      "  n: 13\n",
      "  Steps: 29\n",
      "  Total reward: -189.39999999999995\n",
      "  Final T: [0 0 0 1 0 0 0 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 4. Test on ALL fixed graphs\n",
    "# =========================\n",
    "print(\"\\nRunning inference on fixed test graphs...\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for idx, (A_test, T_test) in enumerate(TEST_GRAPHS):\n",
    "    test_env = GraphEnv(A_test.copy(), T_test.copy())\n",
    "\n",
    "    result = run_inference(\n",
    "        test_env,\n",
    "        encoder,\n",
    "        policy,\n",
    "        deterministic=True\n",
    "    )\n",
    "\n",
    "    all_results.append({\n",
    "        \"graph_id\": idx,\n",
    "        \"solved\": result[\"done\"],\n",
    "        \"steps\": result[\"steps\"],\n",
    "        \"cost\": -result[\"total_reward\"]\n",
    "    })\n",
    "\n",
    "    print(f\"\\nTest Graph {idx}\")\n",
    "    print(\"  n:\", A_test.shape[0])\n",
    "    print(\"  Steps:\", result[\"steps\"])\n",
    "    print(\"  Total reward:\", result[\"total_reward\"])\n",
    "    print(\"  Final T:\", test_env.T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
