{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd110c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "\n",
    "############################################\n",
    "# Constants\n",
    "############################################\n",
    "t1, t2, t3, b = 0.1, 0.1, 10.0, 0.5\n",
    "ACTION_COST = {\n",
    "    1: 1*t1 + 4*t2 + b*t3,\n",
    "    2: 1*t1 + 1*t2,\n",
    "    3: 1*t1 + 1*t2,\n",
    "    4: 1*t1 + 3*t2,\n",
    "    5: 1*t3,\n",
    "    6: 3*t2 + 1*t3\n",
    "}\n",
    "\n",
    "############################################\n",
    "# Graph Environment\n",
    "############################################\n",
    "class GraphEnv:\n",
    "    def __init__(self, A, T=None):\n",
    "        self.reset(A, T)\n",
    "\n",
    "    def reset(self, A, T=None):\n",
    "        self.A = A.copy()\n",
    "        self.n = A.shape[0]\n",
    "        if T is None:\n",
    "            self.T = np.full(self.n, -1, dtype=int)\n",
    "        else:\n",
    "            self.T = T.copy()\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_neighbors(self, i):\n",
    "        return set(np.where(self.A[i] == 1)[0])\n",
    "\n",
    "    def is_done(self):\n",
    "        return np.all(self.A == 0) and np.all(np.isin(self.T, [0, 1]))\n",
    "\n",
    "    def delete_node(self, i):\n",
    "        self.T[i] = 0\n",
    "        self.A[i, :] = 0\n",
    "        self.A[:, i] = 0\n",
    "\n",
    "    def valid_actions(self):\n",
    "        actions = []\n",
    "        for i in range(self.n):\n",
    "            Ni = self.get_neighbors(i)\n",
    "            if self.T[i] == -1:\n",
    "                actions.append((1, i, -1))\n",
    "            if self.T[i] == 1 and len(Ni) == 1:\n",
    "                j = next(iter(Ni))\n",
    "                if self.T[j] == -1:\n",
    "                    actions.append((2, i, j))\n",
    "            if self.T[i] == -1 and len(Ni) == 1:\n",
    "                j = next(iter(Ni))\n",
    "                if self.T[j] == 1:\n",
    "                    actions.append((3, i, j))\n",
    "            for j in range(i + 1, self.n):\n",
    "                if self.get_neighbors(i) == self.get_neighbors(j):\n",
    "                    if self.T[i] == -1 and self.T[j] == 1:\n",
    "                        actions.append((4, i, j))\n",
    "                if self.T[i] == 1 and self.T[j] == 1:\n",
    "                    if self.A[i, j] == 1:\n",
    "                        actions.append((5, i, j))\n",
    "                    if self.get_neighbors(i) == self.get_neighbors(j):\n",
    "                        actions.append((6, i, j))\n",
    "        return list(set(actions))\n",
    "\n",
    "    def step(self, action):\n",
    "        a, i, j = action\n",
    "        cost = ACTION_COST[a]\n",
    "        if a == 1:\n",
    "            self.T[i] = 1\n",
    "        elif a == 2:\n",
    "            self.T[j] = 1\n",
    "            self.delete_node(i)\n",
    "        elif a == 3:\n",
    "            self.delete_node(i)\n",
    "        elif a == 4:\n",
    "            self.delete_node(i)\n",
    "        elif a == 5:\n",
    "            self.A[i, j] = self.A[j, i] = 0\n",
    "        elif a == 6:\n",
    "            self.delete_node(i)\n",
    "        reward = -cost\n",
    "        return self.get_state(), reward, self.is_done()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.A.copy(), self.T.copy()\n",
    "\n",
    "############################################\n",
    "# GIN Encoder\n",
    "############################################\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=3, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        def mlp():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "        self.lin_in = nn.Linear(in_dim, hidden_dim)\n",
    "        self.conv1 = GINConv(mlp())\n",
    "        self.conv2 = GINConv(mlp())\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.lin_in(batch.x)\n",
    "        x = self.conv1(x, batch.edge_index)\n",
    "        x = self.conv2(x, batch.edge_index)\n",
    "        graph_emb = global_mean_pool(x, batch.batch)\n",
    "        return x, graph_emb\n",
    "\n",
    "############################################\n",
    "# Q Network\n",
    "############################################\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(128 + 262, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    def forward(self, graph_emb, action_emb):\n",
    "        return self.net(torch.cat([graph_emb, action_emb], dim=1))\n",
    "\n",
    "############################################\n",
    "# Replay Buffer\n",
    "############################################\n",
    "Transition = namedtuple(\"Transition\", \"state action reward next_state done\")\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, cap):\n",
    "        self.buffer = deque(maxlen=cap)\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "############################################\n",
    "# Utilities\n",
    "############################################\n",
    "def graph_to_data(A, T):\n",
    "    x = torch.zeros((len(T), 3))\n",
    "    for i, t in enumerate(T):\n",
    "        x[i, t + 1] = 1\n",
    "    edge_index = torch.tensor(np.array(np.where(A == 1)), dtype=torch.long)\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "def encode_action(action, node_emb, device):\n",
    "    a, i, j = action\n",
    "    vec = torch.zeros(262, device=device)\n",
    "    vec[a - 1] = 1\n",
    "    vec[6:6+128] = node_emb[i]\n",
    "    if j != -1:\n",
    "        vec[6+128:] = node_emb[j]\n",
    "    return vec\n",
    "\n",
    "############################################\n",
    "# Training Loop\n",
    "############################################\n",
    "def train(envs, episodes=300):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    encoder = GINEncoder().to(device)\n",
    "    qnet = QNetwork().to(device)\n",
    "    target = QNetwork().to(device)\n",
    "    target.load_state_dict(qnet.state_dict())\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(qnet.parameters()), lr=1e-3)\n",
    "    buffer = ReplayBuffer(10000)\n",
    "    eps = 1.0\n",
    "    gamma = 0.99\n",
    "    step = 0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        env = random.choice(envs)\n",
    "        state = env.reset(env.A, env.T)\n",
    "        done = False\n",
    "        while not done:\n",
    "            A, T = state\n",
    "            data = graph_to_data(A, T)\n",
    "            batch = Batch.from_data_list([data]).to(device)\n",
    "            node_emb, graph_emb = encoder(batch)\n",
    "            actions = env.valid_actions()\n",
    "\n",
    "            if random.random() < eps:\n",
    "                action = random.choice(actions)\n",
    "            else:\n",
    "                qvals = [qnet(graph_emb, encode_action(a, node_emb, device).unsqueeze(0)).item() for a in actions]\n",
    "                action = actions[np.argmax(qvals)]\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            # Training from buffer\n",
    "            if len(buffer) >= 256:\n",
    "                batch_tr = buffer.sample(256)\n",
    "                graphs, actions_b, rewards, next_graphs, dones = zip(*batch_tr)\n",
    "                data_list = [graph_to_data(*s) for s in graphs]\n",
    "                batch_data = Batch.from_data_list(data_list).to(device)\n",
    "                node_embs, graph_embs = encoder(batch_data)\n",
    "                action_embs = torch.stack([encode_action(actions_b[i], node_embs, device) for i in range(256)])\n",
    "                q = qnet(graph_embs, action_embs).squeeze()\n",
    "                with torch.no_grad():\n",
    "                    target_q = []\n",
    "                    for i in range(256):\n",
    "                        if dones[i]:\n",
    "                            target_q.append(rewards[i])\n",
    "                        else:\n",
    "                            A2, T2 = next_graphs[i]\n",
    "                            env2 = GraphEnv(A2, T2)\n",
    "                            acts2 = env2.valid_actions()\n",
    "                            d2 = graph_to_data(A2, T2)\n",
    "                            b2 = Batch.from_data_list([d2]).to(device)\n",
    "                            ne2, ge2 = encoder(b2)\n",
    "                            maxq = max(target(ge2, encode_action(a2, ne2, device).unsqueeze(0)).item() for a2 in acts2)\n",
    "                            target_q.append(rewards[i] + gamma * maxq)\n",
    "                    target_q = torch.tensor(target_q, device=device)\n",
    "                loss = nn.functional.mse_loss(q, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if step % 500 == 0:\n",
    "                target.load_state_dict(qnet.state_dict())\n",
    "            step += 1\n",
    "        eps *= 0.99\n",
    "        print(f\"Episode {ep+1}/{episodes}, epsilon={eps:.3f}\")\n",
    "    return encoder, qnet\n",
    "\n",
    "############################################\n",
    "# Inference\n",
    "############################################\n",
    "def run_inference(env, encoder, qnet):\n",
    "    device = next(qnet.parameters()).device\n",
    "    state = env.get_state()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while not env.is_done():\n",
    "        A, T = state\n",
    "        data = graph_to_data(A, T)\n",
    "        batch = Batch.from_data_list([data]).to(device)\n",
    "        node_emb, graph_emb = encoder(batch)\n",
    "        actions = env.valid_actions()\n",
    "        qvals = [qnet(graph_emb, encode_action(a, node_emb, device).unsqueeze(0)).item() for a in actions]\n",
    "        action = actions[np.argmax(qvals)]\n",
    "        state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "    return {\"done\": env.is_done(), \"steps\": steps, \"total_reward\": total_reward, \"final_T\": env.T, \"final_A\": env.A}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a88e25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Load fixed graphs\n",
    "with open(\"fixed_graph_dataset.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "TRAIN_GRAPHS = data[\"train\"]\n",
    "TEST_GRAPHS  = data[\"test\"]\n",
    "\n",
    "def make_envs(graphs):\n",
    "    return [GraphEnv(A.copy(), T.copy()) for A, T in graphs]\n",
    "\n",
    "# Training envs\n",
    "train_envs = make_envs(TRAIN_GRAPHS)\n",
    "print(\"Creating training graphs...\")\n",
    "for k, (A, _) in enumerate(TRAIN_GRAPHS):\n",
    "    print(f\"Train graph {k}: n={A.shape[0]}\")\n",
    "\n",
    "# Train DQN\n",
    "print(\"\\nTraining DQN...\")\n",
    "encoder, qnet = train(train_envs, episodes=300)\n",
    "\n",
    "# Save checkpoint\n",
    "torch.save({\"encoder_state\": encoder.state_dict(), \"qnet_state\": qnet.state_dict()}, \"graph_dqn_checkpoint.pt\")\n",
    "print(\"Model saved to graph_dqn_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b2728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint for inference\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = GINEncoder().to(device)\n",
    "qnet = QNetwork().to(device)\n",
    "checkpoint = torch.load(\"graph_dqn_checkpoint.pt\", map_location=device)\n",
    "encoder.load_state_dict(checkpoint[\"encoder_state\"])\n",
    "qnet.load_state_dict(checkpoint[\"qnet_state\"])\n",
    "encoder.eval()\n",
    "qnet.eval()\n",
    "print(\"\\nModel loaded successfully\")\n",
    "\n",
    "# Inference on all test graphs\n",
    "print(\"\\nRunning inference on ALL fixed test graphs...\")\n",
    "all_results = []\n",
    "\n",
    "for idx, (A_test, T_test) in enumerate(TEST_GRAPHS):\n",
    "    test_env = GraphEnv(A_test.copy(), T_test.copy())\n",
    "    result = run_inference(test_env, encoder, qnet)\n",
    "    all_results.append({\n",
    "        \"graph_id\": idx,\n",
    "        \"solved\": result[\"done\"],\n",
    "        \"steps\": result[\"steps\"],\n",
    "        \"cost\": -result[\"total_reward\"]\n",
    "    })\n",
    "    print(f\"\\nTest Graph {idx}\")\n",
    "    print(\"  n:\", A_test.shape[0])\n",
    "    print(\"  Steps:\", result[\"steps\"])\n",
    "    print(\"  Total reward:\", result[\"total_reward\"])\n",
    "    print(\"  Final T:\", result[\"final_T\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
