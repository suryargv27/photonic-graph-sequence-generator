{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bd110c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "\n",
    "############################################\n",
    "# Constants\n",
    "############################################\n",
    "t1, t2, t3, b = 0.1, 0.1, 10.0, 0.5\n",
    "\n",
    "ACTION_COST = {\n",
    "    1: 1*t1 + 4*t2 + b*t3,\n",
    "    2: 1*t1 + 1*t2,\n",
    "    3: 1*t1 + 1*t2,\n",
    "    4: 1*t1 + 3*t2,\n",
    "    5: 1*t3,\n",
    "    6: 3*t2 + 1*t3\n",
    "}\n",
    "\n",
    "############################################\n",
    "# Graph Environment\n",
    "############################################\n",
    "class GraphEnv:\n",
    "    def __init__(self, A):\n",
    "        self.reset(A)\n",
    "\n",
    "    def reset(self, A):\n",
    "        self.A = A.copy()\n",
    "        self.n = A.shape[0]\n",
    "        self.T = np.full(self.n, -1, dtype=int)\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_neighbors(self, i):\n",
    "        return set(np.where(self.A[i] == 1)[0])\n",
    "\n",
    "    def is_done(self):\n",
    "        return np.all(self.A == 0) and np.all(np.isin(self.T, [0, 1]))\n",
    "\n",
    "    def delete_node(self, i):\n",
    "        self.T[i] = 0\n",
    "        self.A[i, :] = 0\n",
    "        self.A[:, i] = 0\n",
    "\n",
    "    def valid_actions(self):\n",
    "        actions = []\n",
    "        for i in range(self.n):\n",
    "            Ni = self.get_neighbors(i)\n",
    "\n",
    "            if self.T[i] == -1:\n",
    "                actions.append((1, i, -1))\n",
    "\n",
    "            if self.T[i] == 1 and len(Ni) == 1:\n",
    "                j = next(iter(Ni))\n",
    "                if self.T[j] == -1:\n",
    "                    actions.append((2, i, j))\n",
    "\n",
    "            if self.T[i] == -1 and len(Ni) == 1:\n",
    "                j = next(iter(Ni))\n",
    "                if self.T[j] == 1:\n",
    "                    actions.append((3, i, j))\n",
    "\n",
    "            for j in range(i + 1, self.n):\n",
    "                if self.get_neighbors(i) == self.get_neighbors(j):\n",
    "                    if self.T[i] == -1 and self.T[j] == 1:\n",
    "                        actions.append((4, i, j))\n",
    "\n",
    "                if self.T[i] == 1 and self.T[j] == 1:\n",
    "                    if self.A[i, j] == 1:\n",
    "                        actions.append((5, i, j))\n",
    "                    if self.get_neighbors(i) == self.get_neighbors(j):\n",
    "                        actions.append((6, i, j))\n",
    "\n",
    "        return list(set(actions))\n",
    "\n",
    "    def step(self, action):\n",
    "        a, i, j = action\n",
    "        cost = ACTION_COST[a]\n",
    "\n",
    "        if a == 1:\n",
    "            self.T[i] = 1\n",
    "        elif a == 2:\n",
    "            self.T[j] = 1\n",
    "            self.delete_node(i)\n",
    "        elif a == 3:\n",
    "            self.delete_node(i)\n",
    "        elif a == 4:\n",
    "            self.delete_node(i)\n",
    "        elif a == 5:\n",
    "            self.A[i, j] = self.A[j, i] = 0\n",
    "        elif a == 6:\n",
    "            self.delete_node(i)\n",
    "\n",
    "        reward = -cost\n",
    "        return self.get_state(), reward, self.is_done()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.A.copy(), self.T.copy()\n",
    "\n",
    "############################################\n",
    "# GNN Encoder (GIN)\n",
    "############################################\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=3, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        def mlp():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "\n",
    "        self.lin_in = nn.Linear(in_dim, hidden_dim)\n",
    "        self.conv1 = GINConv(mlp())\n",
    "        self.conv2 = GINConv(mlp())\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.lin_in(batch.x)\n",
    "        x = self.conv1(x, batch.edge_index)\n",
    "        x = self.conv2(x, batch.edge_index)\n",
    "\n",
    "        graph_emb = global_mean_pool(x, batch.batch)\n",
    "        return x, graph_emb\n",
    "\n",
    "############################################\n",
    "# Q Network (DQN)\n",
    "############################################\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(128 + 262, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, graph_emb, action_emb):\n",
    "        return self.net(torch.cat([graph_emb, action_emb], dim=1))\n",
    "\n",
    "############################################\n",
    "# Replay Buffer\n",
    "############################################\n",
    "Transition = namedtuple(\"Transition\", \"state action reward next_state done\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, cap):\n",
    "        self.buffer = deque(maxlen=cap)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "############################################\n",
    "# Utilities\n",
    "############################################\n",
    "def graph_to_data(A, T):\n",
    "    x = torch.zeros((len(T), 3))\n",
    "    for i, t in enumerate(T):\n",
    "        x[i, t + 1] = 1\n",
    "\n",
    "    edge_index = torch.tensor(np.array(np.where(A == 1)), dtype=torch.long)\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "def encode_action(action, node_emb, device):\n",
    "    a, i, j = action\n",
    "    vec = torch.zeros(262, device=device)\n",
    "    vec[a - 1] = 1\n",
    "    vec[6:6+128] = node_emb[i]\n",
    "    if j != -1:\n",
    "        vec[6+128:] = node_emb[j]\n",
    "    return vec\n",
    "\n",
    "############################################\n",
    "# Training Loop\n",
    "############################################\n",
    "def train(envs, episodes=300):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    encoder = GINEncoder().to(device)\n",
    "    qnet = QNetwork().to(device)\n",
    "    target = QNetwork().to(device)\n",
    "    target.load_state_dict(qnet.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        list(encoder.parameters()) + list(qnet.parameters()), lr=1e-3\n",
    "    )\n",
    "\n",
    "    buffer = ReplayBuffer(10000)\n",
    "\n",
    "    eps = 1.0\n",
    "    gamma = 0.99\n",
    "    step = 0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        env = random.choice(envs)\n",
    "        state = env.reset(env.A)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            A, T = state\n",
    "            data = graph_to_data(A, T)\n",
    "            batch = Batch.from_data_list([data]).to(device)\n",
    "\n",
    "            node_emb, graph_emb = encoder(batch)\n",
    "            actions = env.valid_actions()\n",
    "\n",
    "            if random.random() < eps:\n",
    "                action = random.choice(actions)\n",
    "            else:\n",
    "                qvals = []\n",
    "                for act in actions:\n",
    "                    a_emb = encode_action(act, node_emb, device).unsqueeze(0)\n",
    "                    qvals.append(qnet(graph_emb, a_emb).item())\n",
    "                action = actions[np.argmax(qvals)]\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            if len(buffer) >= 256:\n",
    "                batch_tr = buffer.sample(256)\n",
    "\n",
    "                graphs, actions_b, rewards, next_graphs, dones = zip(*batch_tr)\n",
    "\n",
    "                data_list = [graph_to_data(*s) for s in graphs]\n",
    "                batch_data = Batch.from_data_list(data_list).to(device)\n",
    "\n",
    "                node_embs, graph_embs = encoder(batch_data)\n",
    "\n",
    "                action_embs = torch.stack([\n",
    "                    encode_action(actions_b[i], node_embs, device)\n",
    "                    for i in range(256)\n",
    "                ])\n",
    "\n",
    "                q = qnet(graph_embs, action_embs).squeeze()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    target_q = []\n",
    "                    for i in range(256):\n",
    "                        if dones[i]:\n",
    "                            target_q.append(rewards[i])\n",
    "                        else:\n",
    "                            A2, T2 = next_graphs[i]\n",
    "                            env2 = GraphEnv(A2)\n",
    "                            env2.T = T2.copy()\n",
    "                            acts2 = env2.valid_actions()\n",
    "\n",
    "                            d2 = graph_to_data(A2, T2)\n",
    "                            b2 = Batch.from_data_list([d2]).to(device)\n",
    "                            ne2, ge2 = encoder(b2)\n",
    "\n",
    "                            maxq = max(\n",
    "                                target(\n",
    "                                    ge2,\n",
    "                                    encode_action(a2, ne2, device).unsqueeze(0)\n",
    "                                ).item()\n",
    "                                for a2 in acts2\n",
    "                            )\n",
    "                            target_q.append(rewards[i] + gamma * maxq)\n",
    "\n",
    "                    target_q = torch.tensor(target_q, device=device)\n",
    "\n",
    "                loss = nn.functional.mse_loss(q, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if step % 500 == 0:\n",
    "                target.load_state_dict(qnet.state_dict())\n",
    "\n",
    "            step += 1\n",
    "\n",
    "        eps *= 0.99\n",
    "        print(f\"Episode {ep+1}/{episodes}, epsilon={eps:.3f}\")\n",
    "\n",
    "    return encoder, qnet\n",
    "\n",
    "############################################\n",
    "# Inference\n",
    "############################################\n",
    "def infer(env, encoder, qnet):\n",
    "    device = next(qnet.parameters()).device\n",
    "    state = env.get_state()\n",
    "\n",
    "    while not env.is_done():\n",
    "        A, T = state\n",
    "        data = graph_to_data(A, T)\n",
    "        batch = Batch.from_data_list([data]).to(device)\n",
    "\n",
    "        node_emb, graph_emb = encoder(batch)\n",
    "        actions = env.valid_actions()\n",
    "\n",
    "        qvals = [\n",
    "            qnet(\n",
    "                graph_emb,\n",
    "                encode_action(a, node_emb, device).unsqueeze(0)\n",
    "            ).item()\n",
    "            for a in actions\n",
    "        ]\n",
    "\n",
    "        action = actions[np.argmax(qvals)]\n",
    "        state, _, _ = env.step(action)\n",
    "\n",
    "    return env.T, env.A\n",
    "\n",
    "def generate_random_connected_graph(n, p=0.3, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    A = np.zeros((n, n), dtype=int)\n",
    "\n",
    "    # Ensure connectivity via random spanning tree\n",
    "    nodes = list(range(n))\n",
    "    random.shuffle(nodes)\n",
    "    for i in range(1, n):\n",
    "        j = random.randint(0, i - 1)\n",
    "        A[nodes[i], nodes[j]] = 1\n",
    "        A[nodes[j], nodes[i]] = 1\n",
    "\n",
    "    # Add extra edges\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if random.random() < p:\n",
    "                A[i, j] = 1\n",
    "                A[j, i] = 1\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b12c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# -----------------------------\n",
    "# Create training environments\n",
    "# -----------------------------\n",
    "envs = []\n",
    "for _ in range(5):\n",
    "    n = random.randint(10,12)   # variable n\n",
    "    A = generate_random_connected_graph(n, p=0.4)\n",
    "    envs.append(GraphEnv(A))\n",
    "\n",
    "# -----------------------------\n",
    "# Train\n",
    "# -----------------------------\n",
    "print(\"Training...\")\n",
    "encoder, qnet = train(envs, episodes=300)\n",
    "\n",
    "checkpoint = {\n",
    "    \"encoder_state\": encoder.state_dict(),\n",
    "    \"qnet_state\": qnet.state_dict(),\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \"graph_dqn_checkpoint.pt\")\n",
    "print(\"Model saved to graph_dqn_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e74e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Test / inference\n",
    "# -----------------------------\n",
    "print(\"\\nRunning inference on unseen graph...\\n\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = GINEncoder().to(device)\n",
    "qnet = QNetwork().to(device)\n",
    "checkpoint = torch.load(\"graph_dqn_checkpoint.pt\", map_location=device)\n",
    "\n",
    "encoder.load_state_dict(checkpoint[\"encoder_state\"])\n",
    "qnet.load_state_dict(checkpoint[\"qnet_state\"])\n",
    "\n",
    "encoder.eval()\n",
    "qnet.eval()\n",
    "\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "n_test = random.randint(8, 14)\n",
    "A_test = generate_random_connected_graph(n_test, p=0.4)\n",
    "test_env = GraphEnv(A_test)\n",
    "\n",
    "final_T, final_A = infer(test_env, encoder, qnet)\n",
    "\n",
    "print(\"Final T:\", final_T)\n",
    "print(\"Final adjacency matrix:\")\n",
    "print(final_A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
