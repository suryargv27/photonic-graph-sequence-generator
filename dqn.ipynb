{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bd110c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DQN...\n",
      "Episode 1/300, epsilon=0.990\n",
      "Episode 2/300, epsilon=0.980\n",
      "Episode 3/300, epsilon=0.970\n",
      "Episode 4/300, epsilon=0.961\n",
      "Episode 5/300, epsilon=0.951\n",
      "Episode 6/300, epsilon=0.941\n",
      "Episode 7/300, epsilon=0.932\n",
      "Episode 8/300, epsilon=0.923\n",
      "Episode 9/300, epsilon=0.914\n",
      "Episode 10/300, epsilon=0.904\n",
      "Episode 11/300, epsilon=0.895\n",
      "Episode 12/300, epsilon=0.886\n",
      "Episode 13/300, epsilon=0.878\n",
      "Episode 14/300, epsilon=0.869\n",
      "Episode 15/300, epsilon=0.860\n",
      "Episode 16/300, epsilon=0.851\n",
      "Episode 17/300, epsilon=0.843\n",
      "Episode 18/300, epsilon=0.835\n",
      "Episode 19/300, epsilon=0.826\n",
      "Episode 20/300, epsilon=0.818\n",
      "Episode 21/300, epsilon=0.810\n",
      "Episode 22/300, epsilon=0.802\n",
      "Episode 23/300, epsilon=0.794\n",
      "Episode 24/300, epsilon=0.786\n",
      "Episode 25/300, epsilon=0.778\n",
      "Episode 26/300, epsilon=0.770\n",
      "Episode 27/300, epsilon=0.762\n",
      "Episode 28/300, epsilon=0.755\n",
      "Episode 29/300, epsilon=0.747\n",
      "Episode 30/300, epsilon=0.740\n",
      "Episode 31/300, epsilon=0.732\n",
      "Episode 32/300, epsilon=0.725\n",
      "Episode 33/300, epsilon=0.718\n",
      "Episode 34/300, epsilon=0.711\n",
      "Episode 35/300, epsilon=0.703\n",
      "Episode 36/300, epsilon=0.696\n",
      "Episode 37/300, epsilon=0.689\n",
      "Episode 38/300, epsilon=0.683\n",
      "Episode 39/300, epsilon=0.676\n",
      "Episode 40/300, epsilon=0.669\n",
      "Episode 41/300, epsilon=0.662\n",
      "Episode 42/300, epsilon=0.656\n",
      "Episode 43/300, epsilon=0.649\n",
      "Episode 44/300, epsilon=0.643\n",
      "Episode 45/300, epsilon=0.636\n",
      "Episode 46/300, epsilon=0.630\n",
      "Episode 47/300, epsilon=0.624\n",
      "Episode 48/300, epsilon=0.617\n",
      "Episode 49/300, epsilon=0.611\n",
      "Episode 50/300, epsilon=0.605\n",
      "Episode 51/300, epsilon=0.599\n",
      "Episode 52/300, epsilon=0.593\n",
      "Episode 53/300, epsilon=0.587\n",
      "Episode 54/300, epsilon=0.581\n",
      "Episode 55/300, epsilon=0.575\n",
      "Episode 56/300, epsilon=0.570\n",
      "Episode 57/300, epsilon=0.564\n",
      "Episode 58/300, epsilon=0.558\n",
      "Episode 59/300, epsilon=0.553\n",
      "Episode 60/300, epsilon=0.547\n",
      "Episode 61/300, epsilon=0.542\n",
      "Episode 62/300, epsilon=0.536\n",
      "Episode 63/300, epsilon=0.531\n",
      "Episode 64/300, epsilon=0.526\n",
      "Episode 65/300, epsilon=0.520\n",
      "Episode 66/300, epsilon=0.515\n",
      "Episode 67/300, epsilon=0.510\n",
      "Episode 68/300, epsilon=0.505\n",
      "Episode 69/300, epsilon=0.500\n",
      "Episode 70/300, epsilon=0.495\n",
      "Episode 71/300, epsilon=0.490\n",
      "Episode 72/300, epsilon=0.485\n",
      "Episode 73/300, epsilon=0.480\n",
      "Episode 74/300, epsilon=0.475\n",
      "Episode 75/300, epsilon=0.471\n",
      "Episode 76/300, epsilon=0.466\n",
      "Episode 77/300, epsilon=0.461\n",
      "Episode 78/300, epsilon=0.457\n",
      "Episode 79/300, epsilon=0.452\n",
      "Episode 80/300, epsilon=0.448\n",
      "Episode 81/300, epsilon=0.443\n",
      "Episode 82/300, epsilon=0.439\n",
      "Episode 83/300, epsilon=0.434\n",
      "Episode 84/300, epsilon=0.430\n",
      "Episode 85/300, epsilon=0.426\n",
      "Episode 86/300, epsilon=0.421\n",
      "Episode 87/300, epsilon=0.417\n",
      "Episode 88/300, epsilon=0.413\n",
      "Episode 89/300, epsilon=0.409\n",
      "Episode 90/300, epsilon=0.405\n",
      "Episode 91/300, epsilon=0.401\n",
      "Episode 92/300, epsilon=0.397\n",
      "Episode 93/300, epsilon=0.393\n",
      "Episode 94/300, epsilon=0.389\n",
      "Episode 95/300, epsilon=0.385\n",
      "Episode 96/300, epsilon=0.381\n",
      "Episode 97/300, epsilon=0.377\n",
      "Episode 98/300, epsilon=0.373\n",
      "Episode 99/300, epsilon=0.370\n",
      "Episode 100/300, epsilon=0.366\n",
      "Episode 101/300, epsilon=0.362\n",
      "Episode 102/300, epsilon=0.359\n",
      "Episode 103/300, epsilon=0.355\n",
      "Episode 104/300, epsilon=0.352\n",
      "Episode 105/300, epsilon=0.348\n",
      "Episode 106/300, epsilon=0.345\n",
      "Episode 107/300, epsilon=0.341\n",
      "Episode 108/300, epsilon=0.338\n",
      "Episode 109/300, epsilon=0.334\n",
      "Episode 110/300, epsilon=0.331\n",
      "Episode 111/300, epsilon=0.328\n",
      "Episode 112/300, epsilon=0.324\n",
      "Episode 113/300, epsilon=0.321\n",
      "Episode 114/300, epsilon=0.318\n",
      "Episode 115/300, epsilon=0.315\n",
      "Episode 116/300, epsilon=0.312\n",
      "Episode 117/300, epsilon=0.309\n",
      "Episode 118/300, epsilon=0.305\n",
      "Episode 119/300, epsilon=0.302\n",
      "Episode 120/300, epsilon=0.299\n",
      "Episode 121/300, epsilon=0.296\n",
      "Episode 122/300, epsilon=0.293\n",
      "Episode 123/300, epsilon=0.290\n",
      "Episode 124/300, epsilon=0.288\n",
      "Episode 125/300, epsilon=0.285\n",
      "Episode 126/300, epsilon=0.282\n",
      "Episode 127/300, epsilon=0.279\n",
      "Episode 128/300, epsilon=0.276\n",
      "Episode 129/300, epsilon=0.273\n",
      "Episode 130/300, epsilon=0.271\n",
      "Episode 131/300, epsilon=0.268\n",
      "Episode 132/300, epsilon=0.265\n",
      "Episode 133/300, epsilon=0.263\n",
      "Episode 134/300, epsilon=0.260\n",
      "Episode 135/300, epsilon=0.257\n",
      "Episode 136/300, epsilon=0.255\n",
      "Episode 137/300, epsilon=0.252\n",
      "Episode 138/300, epsilon=0.250\n",
      "Episode 139/300, epsilon=0.247\n",
      "Episode 140/300, epsilon=0.245\n",
      "Episode 141/300, epsilon=0.242\n",
      "Episode 142/300, epsilon=0.240\n",
      "Episode 143/300, epsilon=0.238\n",
      "Episode 144/300, epsilon=0.235\n",
      "Episode 145/300, epsilon=0.233\n",
      "Episode 146/300, epsilon=0.231\n",
      "Episode 147/300, epsilon=0.228\n",
      "Episode 148/300, epsilon=0.226\n",
      "Episode 149/300, epsilon=0.224\n",
      "Episode 150/300, epsilon=0.221\n",
      "Episode 151/300, epsilon=0.219\n",
      "Episode 152/300, epsilon=0.217\n",
      "Episode 153/300, epsilon=0.215\n",
      "Episode 154/300, epsilon=0.213\n",
      "Episode 155/300, epsilon=0.211\n",
      "Episode 156/300, epsilon=0.208\n",
      "Episode 157/300, epsilon=0.206\n",
      "Episode 158/300, epsilon=0.204\n",
      "Episode 159/300, epsilon=0.202\n",
      "Episode 160/300, epsilon=0.200\n",
      "Episode 161/300, epsilon=0.198\n",
      "Episode 162/300, epsilon=0.196\n",
      "Episode 163/300, epsilon=0.194\n",
      "Episode 164/300, epsilon=0.192\n",
      "Episode 165/300, epsilon=0.190\n",
      "Episode 166/300, epsilon=0.189\n",
      "Episode 167/300, epsilon=0.187\n",
      "Episode 168/300, epsilon=0.185\n",
      "Episode 169/300, epsilon=0.183\n",
      "Episode 170/300, epsilon=0.181\n",
      "Episode 171/300, epsilon=0.179\n",
      "Episode 172/300, epsilon=0.178\n",
      "Episode 173/300, epsilon=0.176\n",
      "Episode 174/300, epsilon=0.174\n",
      "Episode 175/300, epsilon=0.172\n",
      "Episode 176/300, epsilon=0.171\n",
      "Episode 177/300, epsilon=0.169\n",
      "Episode 178/300, epsilon=0.167\n",
      "Episode 179/300, epsilon=0.165\n",
      "Episode 180/300, epsilon=0.164\n",
      "Episode 181/300, epsilon=0.162\n",
      "Episode 182/300, epsilon=0.161\n",
      "Episode 183/300, epsilon=0.159\n",
      "Episode 184/300, epsilon=0.157\n",
      "Episode 185/300, epsilon=0.156\n",
      "Episode 186/300, epsilon=0.154\n",
      "Episode 187/300, epsilon=0.153\n",
      "Episode 188/300, epsilon=0.151\n",
      "Episode 189/300, epsilon=0.150\n",
      "Episode 190/300, epsilon=0.148\n",
      "Episode 191/300, epsilon=0.147\n",
      "Episode 192/300, epsilon=0.145\n",
      "Episode 193/300, epsilon=0.144\n",
      "Episode 194/300, epsilon=0.142\n",
      "Episode 195/300, epsilon=0.141\n",
      "Episode 196/300, epsilon=0.139\n",
      "Episode 197/300, epsilon=0.138\n",
      "Episode 198/300, epsilon=0.137\n",
      "Episode 199/300, epsilon=0.135\n",
      "Episode 200/300, epsilon=0.134\n",
      "Episode 201/300, epsilon=0.133\n",
      "Episode 202/300, epsilon=0.131\n",
      "Episode 203/300, epsilon=0.130\n",
      "Episode 204/300, epsilon=0.129\n",
      "Episode 205/300, epsilon=0.127\n",
      "Episode 206/300, epsilon=0.126\n",
      "Episode 207/300, epsilon=0.125\n",
      "Episode 208/300, epsilon=0.124\n",
      "Episode 209/300, epsilon=0.122\n",
      "Episode 210/300, epsilon=0.121\n",
      "Episode 211/300, epsilon=0.120\n",
      "Episode 212/300, epsilon=0.119\n",
      "Episode 213/300, epsilon=0.118\n",
      "Episode 214/300, epsilon=0.116\n",
      "Episode 215/300, epsilon=0.115\n",
      "Episode 216/300, epsilon=0.114\n",
      "Episode 217/300, epsilon=0.113\n",
      "Episode 218/300, epsilon=0.112\n",
      "Episode 219/300, epsilon=0.111\n",
      "Episode 220/300, epsilon=0.110\n",
      "Episode 221/300, epsilon=0.108\n",
      "Episode 222/300, epsilon=0.107\n",
      "Episode 223/300, epsilon=0.106\n",
      "Episode 224/300, epsilon=0.105\n",
      "Episode 225/300, epsilon=0.104\n",
      "Episode 226/300, epsilon=0.103\n",
      "Episode 227/300, epsilon=0.102\n",
      "Episode 228/300, epsilon=0.101\n",
      "Episode 229/300, epsilon=0.100\n",
      "Episode 230/300, epsilon=0.099\n",
      "Episode 231/300, epsilon=0.098\n",
      "Episode 232/300, epsilon=0.097\n",
      "Episode 233/300, epsilon=0.096\n",
      "Episode 234/300, epsilon=0.095\n",
      "Episode 235/300, epsilon=0.094\n",
      "Episode 236/300, epsilon=0.093\n",
      "Episode 237/300, epsilon=0.092\n",
      "Episode 238/300, epsilon=0.091\n",
      "Episode 239/300, epsilon=0.091\n",
      "Episode 240/300, epsilon=0.090\n",
      "Episode 241/300, epsilon=0.089\n",
      "Episode 242/300, epsilon=0.088\n",
      "Episode 243/300, epsilon=0.087\n",
      "Episode 244/300, epsilon=0.086\n",
      "Episode 245/300, epsilon=0.085\n",
      "Episode 246/300, epsilon=0.084\n",
      "Episode 247/300, epsilon=0.084\n",
      "Episode 248/300, epsilon=0.083\n",
      "Episode 249/300, epsilon=0.082\n",
      "Episode 250/300, epsilon=0.081\n",
      "Episode 251/300, epsilon=0.080\n",
      "Episode 252/300, epsilon=0.079\n",
      "Episode 253/300, epsilon=0.079\n",
      "Episode 254/300, epsilon=0.078\n",
      "Episode 255/300, epsilon=0.077\n",
      "Episode 256/300, epsilon=0.076\n",
      "Episode 257/300, epsilon=0.076\n",
      "Episode 258/300, epsilon=0.075\n",
      "Episode 259/300, epsilon=0.074\n",
      "Episode 260/300, epsilon=0.073\n",
      "Episode 261/300, epsilon=0.073\n",
      "Episode 262/300, epsilon=0.072\n",
      "Episode 263/300, epsilon=0.071\n",
      "Episode 264/300, epsilon=0.070\n",
      "Episode 265/300, epsilon=0.070\n",
      "Episode 266/300, epsilon=0.069\n",
      "Episode 267/300, epsilon=0.068\n",
      "Episode 268/300, epsilon=0.068\n",
      "Episode 269/300, epsilon=0.067\n",
      "Episode 270/300, epsilon=0.066\n",
      "Episode 271/300, epsilon=0.066\n",
      "Episode 272/300, epsilon=0.065\n",
      "Episode 273/300, epsilon=0.064\n",
      "Episode 274/300, epsilon=0.064\n",
      "Episode 275/300, epsilon=0.063\n",
      "Episode 276/300, epsilon=0.062\n",
      "Episode 277/300, epsilon=0.062\n",
      "Episode 278/300, epsilon=0.061\n",
      "Episode 279/300, epsilon=0.061\n",
      "Episode 280/300, epsilon=0.060\n",
      "Episode 281/300, epsilon=0.059\n",
      "Episode 282/300, epsilon=0.059\n",
      "Episode 283/300, epsilon=0.058\n",
      "Episode 284/300, epsilon=0.058\n",
      "Episode 285/300, epsilon=0.057\n",
      "Episode 286/300, epsilon=0.056\n",
      "Episode 287/300, epsilon=0.056\n",
      "Episode 288/300, epsilon=0.055\n",
      "Episode 289/300, epsilon=0.055\n",
      "Episode 290/300, epsilon=0.054\n",
      "Episode 291/300, epsilon=0.054\n",
      "Episode 292/300, epsilon=0.053\n",
      "Episode 293/300, epsilon=0.053\n",
      "Episode 294/300, epsilon=0.052\n",
      "Episode 295/300, epsilon=0.052\n",
      "Episode 296/300, epsilon=0.051\n",
      "Episode 297/300, epsilon=0.051\n",
      "Episode 298/300, epsilon=0.050\n",
      "Episode 299/300, epsilon=0.050\n",
      "Episode 300/300, epsilon=0.049\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GINConv, global_mean_pool\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "############################################\n",
    "# Constants\n",
    "############################################\n",
    "t1, t2, t3, b = 0.1, 0.1, 10.0, 0.5\n",
    "\n",
    "ACTION_COST = {\n",
    "    1: 1*t1 + 4*t2 + b*t3,\n",
    "    2: 1*t1 + 1*t2,\n",
    "    3: 1*t1 + 1*t2,\n",
    "    4: 1*t1 + 3*t2,\n",
    "    5: 1*t3,\n",
    "    6: 3*t2 + 1*t3\n",
    "}\n",
    "\n",
    "############################################\n",
    "# Graph Environment\n",
    "############################################\n",
    "class GraphEnv:\n",
    "    def __init__(self, A):\n",
    "        self.reset(A)\n",
    "\n",
    "    def reset(self, A):\n",
    "        self.A = A.copy()\n",
    "        self.n = A.shape[0]\n",
    "        self.T = np.full(self.n, -1, dtype=int)\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_neighbors(self, i):\n",
    "        return set(np.where(self.A[i] == 1)[0])\n",
    "\n",
    "    def is_done(self):\n",
    "        return np.all(self.A == 0) and np.all(np.isin(self.T, [0, 1]))\n",
    "\n",
    "    def delete_node(self, i):\n",
    "        self.T[i] = 0\n",
    "        self.A[i, :] = 0\n",
    "        self.A[:, i] = 0\n",
    "\n",
    "    def valid_actions(self):\n",
    "        actions = []\n",
    "        for i in range(self.n):\n",
    "            Ni = self.get_neighbors(i)\n",
    "\n",
    "            if self.T[i] == -1:\n",
    "                actions.append((1, i, -1))\n",
    "\n",
    "            if self.T[i] == 1 and len(Ni) == 1:\n",
    "                j = next(iter(Ni))\n",
    "                if self.T[j] == -1:\n",
    "                    actions.append((2, i, j))\n",
    "\n",
    "            if self.T[i] == -1 and len(Ni) == 1:\n",
    "                j = next(iter(Ni))\n",
    "                if self.T[j] == 1:\n",
    "                    actions.append((3, i, j))\n",
    "\n",
    "            for j in range(i + 1, self.n):\n",
    "                if self.get_neighbors(i) == self.get_neighbors(j):\n",
    "                    if self.T[i] == -1 and self.T[j] == 1:\n",
    "                        actions.append((4, i, j))\n",
    "\n",
    "                if self.T[i] == 1 and self.T[j] == 1:\n",
    "                    if self.A[i, j] == 1:\n",
    "                        actions.append((5, i, j))\n",
    "                    if self.get_neighbors(i) == self.get_neighbors(j):\n",
    "                        actions.append((6, i, j))\n",
    "\n",
    "        return list(set(actions))\n",
    "\n",
    "    def step(self, action):\n",
    "        a, i, j = action\n",
    "        cost = ACTION_COST[a]\n",
    "\n",
    "        if a == 1:\n",
    "            self.T[i] = 1\n",
    "        elif a == 2:\n",
    "            self.T[j] = 1\n",
    "            self.delete_node(i)\n",
    "        elif a == 3:\n",
    "            self.delete_node(i)\n",
    "        elif a == 4:\n",
    "            self.delete_node(i)\n",
    "        elif a == 5:\n",
    "            self.A[i, j] = self.A[j, i] = 0\n",
    "        elif a == 6:\n",
    "            self.delete_node(i)\n",
    "\n",
    "        reward = -cost\n",
    "        return self.get_state(), reward, self.is_done()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.A.copy(), self.T.copy()\n",
    "\n",
    "############################################\n",
    "# GNN Encoder\n",
    "############################################\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=3, hidden_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        def mlp():\n",
    "            return nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim)\n",
    "            )\n",
    "\n",
    "        self.lin_in = nn.Linear(in_dim, hidden_dim)\n",
    "        self.conv1 = GINConv(mlp())\n",
    "        self.conv2 = GINConv(mlp())\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x = self.lin_in(batch.x)\n",
    "        x = F.relu(self.conv1(x, batch.edge_index))\n",
    "        x = F.relu(self.conv2(x, batch.edge_index))\n",
    "        graph_emb = global_mean_pool(x, batch.batch)\n",
    "        return x, graph_emb\n",
    "\n",
    "############################################\n",
    "# Q Network\n",
    "############################################\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_dim=262, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, graph_emb, action_emb):\n",
    "        return self.net(torch.cat([graph_emb, action_emb], dim=1))\n",
    "\n",
    "############################################\n",
    "# Replay Buffer\n",
    "############################################\n",
    "Transition = namedtuple(\"Transition\", \"state action reward next_state done\")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, cap):\n",
    "        self.buffer = deque(maxlen=cap)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.buffer.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "############################################\n",
    "# Utilities\n",
    "############################################\n",
    "def graph_to_data(A, T):\n",
    "    x = torch.zeros((len(T), 3))\n",
    "    for i, t in enumerate(T):\n",
    "        x[i, t + 1] = 1\n",
    "    edge_index = torch.tensor(np.array(np.nonzero(A)), dtype=torch.long)\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "def encode_action(action, node_emb, device):\n",
    "    a, i, j = action\n",
    "    vec = torch.zeros(262, device=device)\n",
    "    vec[a - 1] = 1\n",
    "    vec[6:6+128] = node_emb[i]\n",
    "    if j != -1:\n",
    "        vec[6+128:] = node_emb[j]\n",
    "    return vec\n",
    "\n",
    "############################################\n",
    "# DQN Train\n",
    "############################################\n",
    "def train_dqn(train_envs, episodes=300, batch_size=256):\n",
    "    encoder = GINEncoder().to(device)\n",
    "    qnet = QNetwork().to(device)\n",
    "    target = QNetwork().to(device)\n",
    "    target.load_state_dict(qnet.state_dict())\n",
    "\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(qnet.parameters()), lr=1e-3)\n",
    "    buffer = ReplayBuffer(10000)\n",
    "    gamma = 0.99\n",
    "    eps = 1.0\n",
    "    step_count = 0\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        env = random.choice(train_envs)\n",
    "        state = env.reset(env.A)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            A, T = state\n",
    "            data = graph_to_data(A, T)\n",
    "            batch = Batch.from_data_list([data]).to(device)\n",
    "            node_emb, graph_emb = encoder(batch)\n",
    "            actions = env.valid_actions()\n",
    "\n",
    "            if random.random() < eps:\n",
    "                action = random.choice(actions)\n",
    "            else:\n",
    "                qvals = [qnet(graph_emb, encode_action(a, node_emb, device).unsqueeze(0)).item() for a in actions]\n",
    "                action = actions[np.argmax(qvals)]\n",
    "\n",
    "            next_state, reward, done = env.step(action)\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            if len(buffer) >= batch_size:\n",
    "                batch_samples = buffer.sample(batch_size)\n",
    "                states_b, actions_b, rewards_b, next_states_b, dones_b = zip(*batch_samples)\n",
    "\n",
    "                # Encode current states\n",
    "                data_list = [graph_to_data(*s) for s in states_b]\n",
    "                batch_data = Batch.from_data_list(data_list).to(device)\n",
    "                node_embs, graph_embs = encoder(batch_data)\n",
    "                action_embs = torch.stack([encode_action(actions_b[i], node_embs, device) for i in range(batch_size)])\n",
    "                qvals = qnet(graph_embs, action_embs).squeeze()\n",
    "\n",
    "                # Compute targets\n",
    "                targets = []\n",
    "                for i in range(batch_size):\n",
    "                    if dones_b[i]:\n",
    "                        targets.append(rewards_b[i])\n",
    "                    else:\n",
    "                        A2, T2 = next_states_b[i]\n",
    "                        env2 = GraphEnv(A2)\n",
    "                        env2.T = T2.copy()\n",
    "                        acts2 = env2.valid_actions()\n",
    "                        d2 = graph_to_data(A2, T2)\n",
    "                        b2 = Batch.from_data_list([d2]).to(device)\n",
    "                        ne2, ge2 = encoder(b2)\n",
    "                        maxq = max(target(ge2, encode_action(a2, ne2, device).unsqueeze(0)).item() for a2 in acts2)\n",
    "                        targets.append(rewards_b[i] + gamma * maxq)\n",
    "                targets = torch.tensor(targets, device=device)\n",
    "                loss = F.mse_loss(qvals, targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if step_count % 500 == 0:\n",
    "                target.load_state_dict(qnet.state_dict())\n",
    "            step_count += 1\n",
    "\n",
    "        eps *= 0.99\n",
    "        print(f\"Episode {ep+1}/{episodes}, epsilon={eps:.3f}\")\n",
    "\n",
    "    return encoder, qnet\n",
    "\n",
    "############################################\n",
    "# DQN Inference\n",
    "############################################\n",
    "def infer_dqn(env, encoder, qnet):\n",
    "    state = env.get_state()\n",
    "    encoder.eval()\n",
    "    qnet.eval()\n",
    "    total_reward = 0\n",
    "\n",
    "    while not env.is_done():\n",
    "        A, T = state\n",
    "        data = graph_to_data(A, T)\n",
    "        batch = Batch.from_data_list([data]).to(device)\n",
    "        node_emb, graph_emb = encoder(batch)\n",
    "        actions = env.valid_actions()\n",
    "        qvals = [qnet(graph_emb, encode_action(a, node_emb, device).unsqueeze(0)).item() for a in actions]\n",
    "        action = actions[np.argmax(qvals)]\n",
    "        state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "    return {\n",
    "        \"final_T\": env.T,\n",
    "        \"final_A\": env.A,\n",
    "        \"total_reward\": total_reward,\n",
    "        \"done\": env.is_done()\n",
    "    }\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# =========================\n",
    "# 1. Load fixed graphs\n",
    "# =========================\n",
    "with open(\"fixed_graph_dataset.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "TRAIN_GRAPHS = data[\"train\"]\n",
    "TEST_GRAPHS  = data[\"test\"]\n",
    "\n",
    "train_envs = [GraphEnv(A.copy()) for A, _ in TRAIN_GRAPHS]\n",
    "\n",
    "# Test on all test graphs\n",
    "for idx, (A_test, _) in enumerate(TEST_GRAPHS):\n",
    "    test_env = GraphEnv(A_test.copy())\n",
    "\n",
    "print(\"Training DQN...\")\n",
    "encoder, qnet = train_dqn(train_envs, episodes=300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "545bdee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference on test graphs...\n",
      "\n",
      "Test Graph 0\n",
      "  n: 10\n",
      "  Total reward: -123.80000000000001\n",
      "  Final T: [1 0 1 0 1 0 1 0 1 1]\n",
      "\n",
      "Test Graph 1\n",
      "  n: 10\n",
      "  Total reward: -174.6\n",
      "  Final T: [1 0 1 1 0 1 1 1 1 1]\n",
      "\n",
      "Test Graph 2\n",
      "  n: 14\n",
      "  Total reward: -326.4\n",
      "  Final T: [1 1 1 1 1 0 1 1 0 1 1 1 1 1]\n",
      "\n",
      "Test Graph 3\n",
      "  n: 14\n",
      "  Total reward: -225.8\n",
      "  Final T: [1 1 1 1 1 1 1 0 1 0 1 0 0 1]\n",
      "\n",
      "Test Graph 4\n",
      "  n: 13\n",
      "  Total reward: -275.59999999999997\n",
      "  Final T: [1 1 1 1 1 1 1 0 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Test on all test graphs\n",
    "print(\"\\nRunning inference on test graphs...\")\n",
    "all_results = []\n",
    "\n",
    "for idx, (A_test,_) in enumerate(TEST_GRAPHS):\n",
    "    test_env = GraphEnv(A_test.copy())\n",
    "    result = infer_dqn(test_env, encoder, qnet)\n",
    "    all_results.append({\n",
    "        \"graph_id\": idx,\n",
    "        \"solved\": result[\"done\"],\n",
    "        \"cost\": -result[\"total_reward\"],\n",
    "        \"final_T\": result[\"final_T\"]\n",
    "    })\n",
    "\n",
    "    print(f\"\\nTest Graph {idx}\")\n",
    "    print(\"  n:\", A_test.shape[0])\n",
    "    print(\"  Total reward:\", result[\"total_reward\"])\n",
    "    print(\"  Final T:\", result[\"final_T\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
