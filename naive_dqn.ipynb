{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c249caeb-8388-4b47-93fe-1fc2e891186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# =========================\n",
    "# Reproducibility\n",
    "# =========================\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =========================\n",
    "# Graph Environment\n",
    "# =========================\n",
    "class GraphEnv:\n",
    "    def __init__(self, A, T):\n",
    "        self.A = A.copy()\n",
    "        self.T = T.copy()\n",
    "        self.n = len(T)\n",
    "        self.done = False\n",
    "\n",
    "    def clone(self):\n",
    "        return GraphEnv(self.A.copy(), self.T.copy())\n",
    "\n",
    "    def get_neighbors(self, i):\n",
    "        return set(np.where(self.A[i] == 1)[0])\n",
    "\n",
    "    def delete_node(self, i):\n",
    "        self.A[i, :] = 0\n",
    "        self.A[:, i] = 0\n",
    "        self.T[i] = 0\n",
    "\n",
    "    def action_cost(self, a):\n",
    "        t1, t2, t3, b = 0.1, 0.1, 10.0, 0.5\n",
    "        return {\n",
    "            1: t1 + 4*t2 + b*t3,\n",
    "            2: t1 + t2,\n",
    "            3: t1 + t2,\n",
    "            4: t1 + 3*t2,\n",
    "            5: t3,\n",
    "            6: 3*t2 + t3\n",
    "        }[a]\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        a, nodes = action\n",
    "        cost = self.action_cost(a)\n",
    "        if a == 1:\n",
    "            self.T[nodes] = 1\n",
    "        elif a == 2:\n",
    "            i, j = nodes\n",
    "            self.T[j] = 1\n",
    "            self.delete_node(i)\n",
    "        elif a in {3,4,6}:\n",
    "            i, _ = nodes\n",
    "            self.delete_node(i)\n",
    "        elif a == 5:\n",
    "            i, j = nodes\n",
    "            self.A[i,j] = self.A[j,i] = 0\n",
    "        self.done = (np.all(self.T != -1) or np.all(self.A.sum(axis=0)==0) or not self.get_valid_actions())\n",
    "        return -cost, self.done\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        acts = []\n",
    "        for i in range(self.n):\n",
    "            if self.T[i] == -1: acts.append((1, i))\n",
    "            if self.T[i] == 1:\n",
    "                nbrs = self.get_neighbors(i)\n",
    "                if len(nbrs)==1:\n",
    "                    j = next(iter(nbrs))\n",
    "                    if self.T[j]==-1: acts.append((2,(i,j)))\n",
    "            if self.T[i] == -1:\n",
    "                nbrs = self.get_neighbors(i)\n",
    "                if len(nbrs)==1:\n",
    "                    j = next(iter(nbrs))\n",
    "                    if self.T[j]==1: acts.append((3,(i,j)))\n",
    "        for i in range(self.n):\n",
    "            for j in range(i+1,self.n):\n",
    "                if self.T[i]==-1 and self.T[j]==1 and self.get_neighbors(i)==self.get_neighbors(j):\n",
    "                    acts.append((4,(i,j)))\n",
    "                if self.T[i]==1 and self.T[j]==1:\n",
    "                    if self.A[i,j]==1: acts.append((5,(i,j)))\n",
    "                    if self.get_neighbors(i)==self.get_neighbors(j): acts.append((6,(i,j)))\n",
    "        return acts\n",
    "\n",
    "    def get_state(self):\n",
    "        return np.concatenate([self.A.flatten(), self.T])\n",
    "\n",
    "# =========================\n",
    "# Action encoding\n",
    "# =========================\n",
    "def encode_action(action, n):\n",
    "    a, nodes = action\n",
    "    v = np.zeros(2*n + 6)\n",
    "    v[a-1] = 1\n",
    "    if isinstance(nodes,int):\n",
    "        v[6+nodes] = 1\n",
    "    else:\n",
    "        i,j = nodes\n",
    "        v[6+i] = 1\n",
    "        v[6+n+j] = 1\n",
    "    return v\n",
    "\n",
    "# =========================\n",
    "# DQN Model\n",
    "# =========================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,1)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "# =========================\n",
    "# Training\n",
    "# =========================\n",
    "def train_dqn(envs, episodes=300, batch_size=64, gamma=0.99, lr=1e-3):\n",
    "    n = envs[0].n\n",
    "    input_dim = n**2 + n + 2*n + 6\n",
    "\n",
    "    q_net = DQN(input_dim).to(device)\n",
    "    target_net = DQN(input_dim).to(device)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=20000)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    eps_start, eps_end, eps_decay = 1.0, 0.05, 0.992\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        env = random.choice(envs).clone()\n",
    "        state = env.get_state()\n",
    "        eps = max(eps_end, eps_start*(eps_decay**ep))\n",
    "        done=False\n",
    "        while not done:\n",
    "            acts = env.get_valid_actions()\n",
    "            if not acts: break\n",
    "            if random.random()<eps:\n",
    "                action = random.choice(acts)\n",
    "            else:\n",
    "                q_net.eval()\n",
    "                with torch.no_grad():\n",
    "                    st_tensor = torch.tensor(state,dtype=torch.float32).repeat(len(acts),1)\n",
    "                    ac_tensor = torch.tensor([encode_action(a,n) for a in acts],dtype=torch.float32)\n",
    "                    inputs = torch.cat([st_tensor, ac_tensor],dim=1).to(device)\n",
    "                    qs = q_net(inputs)\n",
    "                    action = acts[torch.argmax(qs).item()]\n",
    "                q_net.train()\n",
    "            reward, done = env.apply_action(action)\n",
    "            next_state = env.get_state()\n",
    "            memory.append((state, action, reward, next_state, env.get_valid_actions(), done))\n",
    "            state = next_state\n",
    "\n",
    "            # Train\n",
    "            if len(memory)>=batch_size:\n",
    "                batch = random.sample(memory,batch_size)\n",
    "                s_b, a_b, r_b, ns_b, na_b, d_b = zip(*batch)\n",
    "                s_t = torch.tensor(np.stack(s_b),dtype=torch.float32)\n",
    "                a_t = torch.tensor(np.stack([encode_action(a,n) for a in a_b]),dtype=torch.float32)\n",
    "                curr_q = q_net(torch.cat([s_t,a_t],dim=1).to(device)).squeeze()\n",
    "                target_q = torch.zeros(batch_size,device=device)\n",
    "                with torch.no_grad():\n",
    "                    for i in range(batch_size):\n",
    "                        if d_b[i] or not na_b[i]:\n",
    "                            target_q[i]=r_b[i]\n",
    "                        else:\n",
    "                            ns_rep = torch.tensor(ns_b[i],dtype=torch.float32).repeat(len(na_b[i]),1)\n",
    "                            na_enc = torch.tensor([encode_action(a2,n) for a2 in na_b[i]],dtype=torch.float32)\n",
    "                            next_inputs = torch.cat([ns_rep,na_enc],dim=1).to(device)\n",
    "                            target_q[i] = r_b[i] + gamma*torch.max(target_net(next_inputs))\n",
    "                loss = criterion(curr_q,target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        if ep%10==0: target_net.load_state_dict(q_net.state_dict())\n",
    "        print(f\"Episode {ep+1}/{episodes}, epsilon={eps:.3f}\")\n",
    "    return q_net\n",
    "\n",
    "# =========================\n",
    "# Inference\n",
    "# =========================\n",
    "def run_inference(env, q_net):\n",
    "    env_copy = env.clone()\n",
    "    q_net.eval()\n",
    "    total_reward, steps = 0, 0\n",
    "    while True:\n",
    "        acts = env_copy.get_valid_actions()\n",
    "        if not acts: break\n",
    "        with torch.no_grad():\n",
    "            st_tensor = torch.tensor(env_copy.get_state(),dtype=torch.float32).repeat(len(acts),1)\n",
    "            ac_tensor = torch.tensor([encode_action(a,env_copy.n) for a in acts],dtype=torch.float32)\n",
    "            inputs = torch.cat([st_tensor,ac_tensor],dim=1).to(device)\n",
    "            best_action = acts[torch.argmax(q_net(inputs)).item()]\n",
    "        reward, done = env_copy.apply_action(best_action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        if done: break\n",
    "    solved = np.all(env_copy.T != -1) or np.all(env_copy.A.sum(axis=0)==0)\n",
    "    return {\"solved\":solved,\"steps\":steps,\"total_reward\":total_reward,\"final_T\":env_copy.T,\"final_A\":env_copy.A}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1967d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fixed_graph_dataset.pkl\",\"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "TRAIN_GRAPHS = data[\"train\"]\n",
    "TEST_GRAPHS = data[\"test\"]\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "train_envs = [GraphEnv(A.copy(),T.copy()) for A,T in TRAIN_GRAPHS]\n",
    "for k,(A,_) in enumerate(TRAIN_GRAPHS):\n",
    "    print(f\"Train graph {k}: n={A.shape[0]}\")\n",
    "\n",
    "print(\"\\nTraining DQN...\")\n",
    "qnet = train_dqn(train_envs,episodes=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52265038",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRunning inference on all test graphs...\")\n",
    "for idx,(A_test,T_test) in enumerate(TEST_GRAPHS):\n",
    "    test_env = GraphEnv(A_test.copy(),T_test.copy())\n",
    "    res = run_inference(test_env,qnet)\n",
    "    print(f\"\\nTest Graph {idx}\")\n",
    "    print(\"  n:\", A_test.shape[0])\n",
    "    print(\"  Steps:\", res[\"steps\"])\n",
    "    print(\"  Total reward:\", res[\"total_reward\"])\n",
    "    print(\"  Final T:\", res[\"final_T\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
