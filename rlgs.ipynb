{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7c29371-0fe3-4916-a1ee-27af95138f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/bash: line 1: pip: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c249caeb-8388-4b47-93fe-1fc2e891186c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "\n",
    "# Set device for acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# =========================\n",
    "# Graph Environment\n",
    "# =========================\n",
    "class GraphEnv:\n",
    "    def __init__(self, A, T, t1=0.1, t2=0.1, t3=10, b=0.5):\n",
    "        self.A = A.copy()\n",
    "        self.T = T.copy()\n",
    "        self.n = len(T)\n",
    "        self.t1, self.t2, self.t3, self.b = t1, t2, t3, b\n",
    "        self.done = False\n",
    "\n",
    "    def clone(self):\n",
    "        return copy.deepcopy(self)\n",
    "\n",
    "    def get_neighbors(self, i):\n",
    "        return set(np.where(self.A[i] == 1)[0])\n",
    "\n",
    "    def delete_node(self, i):\n",
    "        self.A[i, :] = 0\n",
    "        self.A[:, i] = 0\n",
    "        self.T[i] = 0\n",
    "\n",
    "    def action_cost(self, a):\n",
    "        if a == 1: return self.t1 + 4*self.t2 + self.b*self.t3\n",
    "        if a == 2: return self.t1 + self.t2\n",
    "        if a == 3: return self.t1 + self.t2\n",
    "        if a == 4: return self.t1 + 3*self.t2\n",
    "        if a == 5: return self.t3\n",
    "        if a == 6: return 3*self.t2 + self.t3\n",
    "        return 0\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        a, nodes = action\n",
    "        cost = self.action_cost(a)\n",
    "        \n",
    "        if a == 1:\n",
    "            self.T[nodes] = 1\n",
    "        elif a == 2:\n",
    "            i, j = nodes\n",
    "            self.T[j] = 1\n",
    "            self.delete_node(i)\n",
    "        elif a in {3, 4, 6}:\n",
    "            i, _ = nodes\n",
    "            self.delete_node(i)\n",
    "        elif a == 5:\n",
    "            i, j = nodes\n",
    "            self.A[i, j] = self.A[j, i] = 0\n",
    "\n",
    "        reward = -cost\n",
    "        \n",
    "        self.done = (\n",
    "                np.all(self.T != -1) # All nodes are either 0 (deleted) or 1 (processed)\n",
    "                or np.all(self.A.sum(axis=0) == 0)\n",
    "                or len(self.get_valid_actions()) == 0\n",
    "            )\n",
    "        return reward, self.done\n",
    "\n",
    "    def get_valid_actions(self):\n",
    "        acts = []\n",
    "        n = self.n\n",
    "        for i in range(n):\n",
    "            if self.T[i] == -1:\n",
    "                acts.append((1, i))\n",
    "            if self.T[i] == 1:\n",
    "                nbrs = self.get_neighbors(i)\n",
    "                if len(nbrs) == 1:\n",
    "                    j = next(iter(nbrs))\n",
    "                    if self.T[j] == -1: acts.append((2, (i, j)))\n",
    "            if self.T[i] == -1:\n",
    "                nbrs = self.get_neighbors(i)\n",
    "                if len(nbrs) == 1:\n",
    "                    j = next(iter(nbrs))\n",
    "                    if self.T[j] == 1: acts.append((3, (i, j)))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if self.T[i] == -1 and self.T[j] == 1:\n",
    "                    if self.get_neighbors(i) == self.get_neighbors(j):\n",
    "                        acts.append((4, (i, j)))\n",
    "                if self.T[i] == 1 and self.T[j] == 1:\n",
    "                    if self.A[i, j] == 1: acts.append((5, (i, j)))\n",
    "                    if self.get_neighbors(i) == self.get_neighbors(j):\n",
    "                        acts.append((6, (i, j)))\n",
    "        return acts\n",
    "\n",
    "    def get_state(self):\n",
    "        return np.concatenate([self.A.flatten(), self.T])\n",
    "\n",
    "# =========================\n",
    "# Shared Action Encoding\n",
    "# =========================\n",
    "def encode_action(action, n):\n",
    "    a, nodes = action\n",
    "    v = np.zeros(2*n + 6)\n",
    "    v[a-1] = 1\n",
    "    if isinstance(nodes, (int, np.integer)):\n",
    "        v[6 + nodes] = 1\n",
    "    else:\n",
    "        i, j = nodes\n",
    "        v[6 + i] = 1\n",
    "        v[6 + n + j] = 1\n",
    "    return v\n",
    "\n",
    "# =========================\n",
    "# Q Network\n",
    "# =========================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# =========================\n",
    "# Optimized Training\n",
    "# =========================\n",
    "def train(env, episodes=300, batch_size=64, gamma=0.99, lr=1e-3):\n",
    "    n = env.n\n",
    "    state_dim = n**2 + n\n",
    "    input_dim = state_dim + 2*n + 6\n",
    "\n",
    "    q_net = DQN(input_dim).to(device)\n",
    "    target_net = DQN(input_dim).to(device)\n",
    "    target_net.load_state_dict(q_net.state_dict())\n",
    "    \n",
    "    optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "    memory = deque(maxlen=20000)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    eps_start, eps_end, eps_decay = 1.0, 0.05, 0.992\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        e = env.clone()\n",
    "        state = e.get_state()\n",
    "        eps = max(eps_end, eps_start * (eps_decay ** ep))\n",
    "        ep_reward = 0\n",
    "\n",
    "        while True:\n",
    "            acts = e.get_valid_actions()\n",
    "            if not acts: break\n",
    "\n",
    "            # Epsilon-Greedy Selection\n",
    "            if random.random() < eps:\n",
    "                action = random.choice(acts)\n",
    "            else:\n",
    "                q_net.eval()\n",
    "                with torch.no_grad():\n",
    "                    # Batch predict all valid actions for current state\n",
    "                    st_tensor = torch.tensor(state, dtype=torch.float32).repeat(len(acts), 1)\n",
    "                    ac_tensor = torch.tensor([encode_action(a, n) for a in acts], dtype=torch.float32)\n",
    "                    inputs = torch.cat([st_tensor, ac_tensor], dim=1).to(device)\n",
    "                    qs = q_net(inputs)\n",
    "                    action = acts[torch.argmax(qs).item()]\n",
    "                q_net.train()\n",
    "\n",
    "            reward, done = e.apply_action(action)\n",
    "            next_state = e.get_state()\n",
    "            next_acts = e.get_valid_actions()\n",
    "            \n",
    "            memory.append((state, action, reward, next_state, next_acts, done))\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "\n",
    "            # Vectorized Learning Step\n",
    "            if len(memory) >= batch_size:\n",
    "                batch = random.sample(memory, batch_size)\n",
    "                s_b, a_b, r_b, ns_b, na_b, d_b = zip(*batch)\n",
    "\n",
    "                # 1. Current Q values\n",
    "                s_t = torch.tensor(np.stack(s_b), dtype=torch.float32)\n",
    "                a_t = torch.tensor(np.stack([encode_action(a, n) for a in a_b]), dtype=torch.float32)\n",
    "                curr_inputs = torch.cat([s_t, a_t], dim=1).to(device)\n",
    "                curr_q = q_net(curr_inputs).squeeze()\n",
    "\n",
    "                # 2. Target Q values (Vectorized where possible)\n",
    "                target_q = torch.zeros(batch_size, device=device)\n",
    "                with torch.no_grad():\n",
    "                    for i in range(batch_size):\n",
    "                        if d_b[i] or not na_b[i]:\n",
    "                            target_q[i] = r_b[i]\n",
    "                        else:\n",
    "                            # Sub-batch for next actions\n",
    "                            ns_rep = torch.tensor(ns_b[i], dtype=torch.float32).repeat(len(na_b[i]), 1)\n",
    "                            na_enc = torch.tensor([encode_action(a2, n) for a2 in na_b[i]], dtype=torch.float32)\n",
    "                            next_inputs = torch.cat([ns_rep, na_enc], dim=1).to(device)\n",
    "                            max_next_q = torch.max(target_net(next_inputs))\n",
    "                            target_q[i] = r_b[i] + gamma * max_next_q\n",
    "\n",
    "                loss = criterion(curr_q, target_q)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if done: break\n",
    "\n",
    "        if ep % 10 == 0:\n",
    "            target_net.load_state_dict(q_net.state_dict())\n",
    "\n",
    "    return q_net\n",
    "\n",
    "# =========================\n",
    "# Fixed Inference\n",
    "# =========================\n",
    "def infer_dqn(env, trained_qnet):\n",
    "    env_copy = env.clone()\n",
    "    trained_qnet.eval()\n",
    "    trained_qnet.to(device)\n",
    "    \n",
    "    total_cost = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while True:\n",
    "        valid_actions = env_copy.get_valid_actions()\n",
    "        if not valid_actions: break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            state = env_copy.get_state()\n",
    "            st_tensor = torch.tensor(state, dtype=torch.float32).repeat(len(valid_actions), 1)\n",
    "            ac_tensor = torch.tensor([encode_action(a, env_copy.n) for a in valid_actions], dtype=torch.float32)\n",
    "            inputs = torch.cat([st_tensor, ac_tensor], dim=1).to(device)\n",
    "            \n",
    "            qs = trained_qnet(inputs)\n",
    "            best_idx = torch.argmax(qs).item()\n",
    "            best_action = valid_actions[best_idx]\n",
    "        \n",
    "        reward, done = env_copy.apply_action(best_action)\n",
    "        # Re-extract the actual cost from the shaped reward for reporting\n",
    "        actual_step_cost = env_copy.action_cost(best_action[0])\n",
    "        total_cost += actual_step_cost\n",
    "        steps += 1\n",
    "        \n",
    "        if done: break\n",
    "\n",
    "    t_processed = int(np.sum(env_copy.T == 1))\n",
    "    return steps, t_processed, total_cost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a443da6a-632a-4e93-8409-24e8938ca81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_solve(env):\n",
    "    env_copy = env.clone()\n",
    "    steps = 0\n",
    "    total_cost = 0\n",
    "    \n",
    "    while True:\n",
    "        valid_actions = env_copy.get_valid_actions()\n",
    "        if not valid_actions:\n",
    "            break\n",
    "\n",
    "        # Choose action with minimum immediate cost\n",
    "        # We use a lambda to sort by the action_cost of the action type (action[0])\n",
    "        best_action = min(valid_actions, key=lambda act: env_copy.action_cost(act[0]))\n",
    "\n",
    "        # Track the actual cost before applying any shaping\n",
    "        actual_cost = env_copy.action_cost(best_action[0])\n",
    "        total_cost += actual_cost\n",
    "        \n",
    "        _, done = env_copy.apply_action(best_action)\n",
    "        steps += 1\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    t_processed = int(np.sum(env_copy.T == 1))\n",
    "    return steps, t_processed, total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2013fe82-b914-4909-8bb4-87cf001cb462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start\n",
      "Training done\n",
      "Results\n",
      "DQN     Time: 10, Emitter: 9, cost: 92.0\n",
      "Greedy  Time: 10, Emitter: 9, cost: 92.0\n",
      "Training start\n",
      "Training done\n",
      "Results\n",
      "DQN     Time: 10, Emitter: 9, cost: 94.0\n",
      "Greedy  Time: 10, Emitter: 10, cost: 100.0\n",
      "Training start\n",
      "Training done\n",
      "Results\n",
      "DQN     Time: 10, Emitter: 7, cost: 80.0\n",
      "Greedy  Time: 10, Emitter: 8, cost: 86.0\n",
      "Training start\n",
      "Training done\n",
      "Results\n",
      "DQN     Time: 12, Emitter: 5, cost: 82.0\n",
      "Greedy  Time: 10, Emitter: 10, cost: 100.0\n",
      "Training start\n",
      "Training done\n",
      "Results\n",
      "DQN     Time: 12, Emitter: 3, cost: 64.0\n",
      "Greedy  Time: 10, Emitter: 7, cost: 76.0\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def random_graph(n, p):\n",
    "    G = nx.erdos_renyi_graph(n, p)\n",
    "    A = nx.to_numpy_array(G, dtype=int)\n",
    "    return A\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(5):\n",
    "        n = 10\n",
    "        A = random_graph(n,p=0.3)\n",
    "        T = np.array([-1] * n)\n",
    "        env = GraphEnv(A, T, t1=1, t2=1, t3=10,b=0.5)\n",
    "    \n",
    "        print(\"Training start\")\n",
    "        trained_model = train(env, episodes=300)\n",
    "        print(\"Training done\")\n",
    "    \n",
    "        dqn_steps, dqn_ones, dqn_cost = infer_dqn(env, trained_model)\n",
    "        greedy_steps, greedy_ones, greedy_cost = greedy_solve(env)\n",
    "    \n",
    "        print(\"Results\")\n",
    "        print(f\"DQN     Time: {dqn_steps}, Emitter: {dqn_ones}, cost: {dqn_cost}\")\n",
    "        print(f\"Greedy  Time: {greedy_steps}, Emitter: {greedy_ones}, cost: {greedy_cost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816103b1-7df7-4f57-8d1c-e9967eb7bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still in progress\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "# PyTorch Geometric imports\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "\n",
    "##############################\n",
    "# Environment\n",
    "##############################\n",
    "class GraphEnv:\n",
    "    def __init__(self, A, T):\n",
    "        \"\"\"\n",
    "        A: adjacency matrix (n x n) numpy array\n",
    "        T: type vector (n,) numpy array with values -1,0,1\n",
    "        \"\"\"\n",
    "        self.A = A.copy()\n",
    "        self.T = T.copy()\n",
    "        self.n = len(T)\n",
    "        self.done = False\n",
    "        \n",
    "    def get_neighbors(self, i):\n",
    "        return set(np.where(self.A[i]==1)[0])\n",
    "    \n",
    "    def get_valid_actions(self):\n",
    "        actions = []\n",
    "        n = self.n\n",
    "        \n",
    "        # Action 1\n",
    "        for i in range(n):\n",
    "            if self.T[i]==-1:\n",
    "                actions.append((1, i, -1))\n",
    "        # Action 2\n",
    "        for i in range(n):\n",
    "            if self.T[i]==1:\n",
    "                N = self.get_neighbors(i)\n",
    "                if len(N)==1:\n",
    "                    j = list(N)[0]\n",
    "                    if self.T[j]==-1:\n",
    "                        actions.append((2, i, j))\n",
    "        # Action 3\n",
    "        for i in range(n):\n",
    "            if self.T[i]==-1:\n",
    "                N = self.get_neighbors(i)\n",
    "                if len(N)==1:\n",
    "                    j = list(N)[0]\n",
    "                    if self.T[j]==1:\n",
    "                        actions.append((3, i, j))\n",
    "        # Action 4\n",
    "        for i in range(n):\n",
    "            if self.T[i]==-1:\n",
    "                for j in range(n):\n",
    "                    if self.T[j]==1 and self.get_neighbors(i)==self.get_neighbors(j):\n",
    "                        actions.append((4, i, j))\n",
    "        # Action 5\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if self.T[i]==1 and self.T[j]==1 and self.A[i,j]==1:\n",
    "                    actions.append((5, i, j))\n",
    "        # Action 6\n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if self.T[i]==1 and self.T[j]==1 and self.get_neighbors(i)==self.get_neighbors(j):\n",
    "                    actions.append((6, i, j))\n",
    "        return actions\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        action: tuple (action_type, i, j)\n",
    "        returns: next_state, reward, done\n",
    "        \"\"\"\n",
    "        t1, t2, t3, b = 0.1, 0.1, 10, 0.5\n",
    "        a, i, j = action\n",
    "        cost = 0\n",
    "        \n",
    "        if a==1:\n",
    "            self.T[i]=1\n",
    "            cost = 1*t1 + 4*t2 + b*t3\n",
    "        elif a==2:\n",
    "            self.T[j]=1\n",
    "            self.T[i]=0\n",
    "            self.A[i,:]=0\n",
    "            self.A[:,i]=0\n",
    "            cost = 1*t1 + 1*t2 + 0*t3\n",
    "        elif a==3:\n",
    "            self.T[i]=0\n",
    "            self.A[i,:]=0\n",
    "            self.A[:,i]=0\n",
    "            cost = 1*t1 + 1*t2 + 0*t3\n",
    "        elif a==4:\n",
    "            self.T[i]=0\n",
    "            self.A[i,:]=0\n",
    "            self.A[:,i]=0\n",
    "            cost = 1*t1 + 3*t2 + 0*t3\n",
    "        elif a==5:\n",
    "            self.A[i,j]=0\n",
    "            self.A[j,i]=0\n",
    "            cost = 0*t1 + 0*t2 + 1*t3\n",
    "        elif a==6:\n",
    "            self.T[i]=0\n",
    "            self.A[i,:]=0\n",
    "            self.A[:,i]=0\n",
    "            cost = 0*t1 + 3*t2 + 1*t3\n",
    "        \n",
    "        done = (np.all(self.A==0) and np.all((self.T==0)|(self.T==1)))\n",
    "        reward = -cost\n",
    "        return self.get_state(), reward, done\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Return PyG Data object\n",
    "        edge_index = np.array(np.nonzero(self.A))\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        \n",
    "        # One-hot encode T\n",
    "        T_onehot = np.zeros((self.n,3))\n",
    "        for i,v in enumerate(self.T):\n",
    "            idx = v+1 # -1->0, 0->1, 1->2\n",
    "            T_onehot[i,idx]=1\n",
    "        x = torch.tensor(T_onehot, dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        return data\n",
    "\n",
    "##############################\n",
    "# GIN + MLP Q-network\n",
    "##############################\n",
    "class GIN_Q(nn.Module):\n",
    "    def __init__(self, n_nodes, hidden_dim=128, action_dim=6):\n",
    "        super(GIN_Q, self).__init__()\n",
    "        self.conv1 = GINConv(nn.Sequential(nn.Linear(3, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
    "        self.conv2 = GINConv(nn.Sequential(nn.Linear(hidden_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.n_nodes = n_nodes\n",
    "        self.action_dim = action_dim\n",
    "        # Output dim = action one-hot + i + j one-hot = 6+n+n\n",
    "        self.out_dim = action_dim + 2*n_nodes\n",
    "        self.head = nn.Linear(hidden_dim, self.out_dim)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        # global pooling\n",
    "        h = global_add_pool(h, torch.zeros(h.size(0),dtype=torch.long))\n",
    "        h = self.fc(h)\n",
    "        q = self.head(h)\n",
    "        return q\n",
    "\n",
    "##############################\n",
    "# Replay Buffer\n",
    "##############################\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=10000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "##############################\n",
    "# Epsilon Greedy Action Selection\n",
    "##############################\n",
    "def select_action(model, state, valid_actions, n_nodes, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(valid_actions)\n",
    "    else:\n",
    "        q_values = model(state)\n",
    "        best_action = None\n",
    "        best_q = -float('inf')\n",
    "        for action in valid_actions:\n",
    "            a, i, j = action\n",
    "            a_onehot = np.zeros(6)\n",
    "            a_onehot[a-1]=1\n",
    "            i_onehot = np.zeros(n_nodes)\n",
    "            i_onehot[i]=1\n",
    "            if j==-1:\n",
    "                j_onehot = np.zeros(n_nodes)\n",
    "            else:\n",
    "                j_onehot = np.zeros(n_nodes)\n",
    "                j_onehot[j]=1\n",
    "            act_vec = np.concatenate([a_onehot, i_onehot, j_onehot])\n",
    "            act_vec = torch.tensor(act_vec,dtype=torch.float)\n",
    "            q_val = torch.dot(q_values.flatten(), act_vec)\n",
    "            if q_val>best_q:\n",
    "                best_q = q_val\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "##############################\n",
    "# Training\n",
    "##############################\n",
    "def train(A_init, T_init, n_nodes, episodes=300, batch_size=256):\n",
    "    env = GraphEnv(A_init, T_init)\n",
    "    model = GIN_Q(n_nodes)\n",
    "    target_model = GIN_Q(n_nodes)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    buffer = ReplayBuffer()\n",
    "    \n",
    "    gamma = 0.99\n",
    "    epsilon = 1.0\n",
    "    epsilon_decay = 0.99\n",
    "    target_update = 500\n",
    "    iter_count = 0\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state = env.get_state()\n",
    "        env.A = A_init.copy()\n",
    "        env.T = T_init.copy()\n",
    "        done=False\n",
    "        while not done:\n",
    "            valid_actions = env.get_valid_actions()\n",
    "            if not valid_actions:\n",
    "                break\n",
    "            action = select_action(model, state, valid_actions, n_nodes, epsilon)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            iter_count+=1\n",
    "            \n",
    "            # Training\n",
    "            if len(buffer) >= batch_size:\n",
    "                states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "                loss = 0\n",
    "                for s,a,r,s2,d in zip(states, actions, rewards, next_states, dones):\n",
    "                    q_pred = model(s)\n",
    "                    # compute target\n",
    "                    valid_actions_next = env.get_valid_actions()\n",
    "                    q_next = 0\n",
    "                    if not d:\n",
    "                        q_next = torch.max(target_model(s2))\n",
    "                    a_onehot = np.zeros(6+n_nodes+n_nodes)\n",
    "                    act,a_i,a_j = a\n",
    "                    a_onehot[act-1]=1\n",
    "                    a_onehot[6+a_i]=1\n",
    "                    if a_j!=-1:\n",
    "                        a_onehot[6+n_nodes+a_j]=1\n",
    "                    a_onehot = torch.tensor(a_onehot,dtype=torch.float)\n",
    "                    q_val = torch.dot(q_pred.flatten(), a_onehot)\n",
    "                    target = r + gamma*q_next\n",
    "                    loss += F.mse_loss(q_val, torch.tensor(target))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            if iter_count % target_update==0:\n",
    "                target_model.load_state_dict(model.state_dict())\n",
    "                \n",
    "        epsilon *= epsilon_decay\n",
    "        print(f\"Episode {ep+1} done. Epsilon: {epsilon:.3f}\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "##############################\n",
    "# Inference\n",
    "##############################\n",
    "def infer(model, A_init, T_init):\n",
    "    env = GraphEnv(A_init, T_init)\n",
    "    state = env.get_state()\n",
    "    done = False\n",
    "    actions_taken=[]\n",
    "    while not done:\n",
    "        valid_actions = env.get_valid_actions()\n",
    "        if not valid_actions:\n",
    "            break\n",
    "        action = select_action(model, state, valid_actions, env.n, epsilon=0.0)\n",
    "        actions_taken.append(action)\n",
    "        state, reward, done = env.step(action)\n",
    "    return actions_taken, env.T, env.A\n",
    "\n",
    "##############################\n",
    "# Example usage\n",
    "##############################\n",
    "if __name__==\"__main__\":\n",
    "    for i in range(5):\n",
    "        n = 5\n",
    "        A = random_graph(n,p=0.3)\n",
    "        T = np.array([-1] * n)\n",
    "    \n",
    "        model = train(A, T, n)\n",
    "        actions, T_final, A_final = infer(model, A, T)\n",
    "        print(\"Actions taken:\", actions)\n",
    "        print(\"Final T:\", T_final)\n",
    "        print(\"Final A:\", A_final)\n",
    "        env = GraphEnv(A, T, t1=1, t2=1, t3=10,b=0.5)\n",
    "\n",
    "        greedy_steps, greedy_ones, greedy_cost = greedy_solve(env)\n",
    "    \n",
    "        print(\"Results\")\n",
    "        print(f\"Greedy  Time: {greedy_steps}, Emitter: {greedy_ones}, cost: {greedy_cost}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59cb19-ed94-4d3d-8858-f7c40983d4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
